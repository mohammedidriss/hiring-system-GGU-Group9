{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiwtr1kvMq1f1WHJoqvJdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedidriss/hiring-system-GGU-Group9/blob/main/Course4_Hiring_system_v1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1"
      ],
      "metadata": {
        "id": "w5cv58GEPa7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkXBCetjbMSw",
        "outputId": "1279ae7d-5bd0-4ead-c75a-201169746a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries: gradio, xgboost, shap, openpyxl, httpx...\n",
            "Installations complete.\n"
          ]
        }
      ],
      "source": [
        "# This cell installs all the external libraries\n",
        "print(\"Installing required libraries: gradio, xgboost, shap, openpyxl, httpx...\")\n",
        "!pip install -q gradio xgboost shap openpyxl httpx\n",
        "print(\"Installations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2"
      ],
      "metadata": {
        "id": "Wf2JtT7zPfLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports all the tools.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "import joblib\n",
        "import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import shap\n",
        "\n",
        "import httpx\n",
        "import asyncio\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported. Ready to mount Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf3tmiZOmjgv",
        "outputId": "6f10c5de-73ef-4ac9-ebc2-1da6290af16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported. Ready to mount Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3"
      ],
      "metadata": {
        "id": "60jpIQ6FPg2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connects Colab notebook to Google Drive\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully at /content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyFxY6-Lmlsw",
        "outputId": "a58817a8-d325-4e67-e758-22db3a9cc84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4"
      ],
      "metadata": {
        "id": "FCUyHL3WPibz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell defines our file paths and creates the \"Master Dataset.\"\n",
        "# It now exclusively uses the 'JobSeeker' sheet to derive features and the 'TARGET_HIRED' label,\n",
        "# and saves this new \"master\" file back to your Drive.\n",
        "\n",
        "print(\"--- Phase 1: Creating Master Training Dataset (JobSeeker only) ---\")\n",
        "\n",
        "# --- 1. Define File Paths ---\n",
        "# This is our original Excel file\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
        "FEATURES_FILE_PATH = f\"{DRIVE_PATH}JDSampleData.xlsx\"\n",
        "FEATURES_SHEET_NAME = \"JobSeeker\"\n",
        "MASTER_DATASET_PATH = f\"{DRIVE_PATH}master_training_dataset.csv\"\n",
        "\n",
        "# --- 2. Define the Function ---\n",
        "def create_master_dataset():\n",
        "    \"\"\"\n",
        "    Loads features from 'JobSeeker' sheet, calculates target labels,\n",
        "    and saves a new \"master\" dataset.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from: {FEATURES_FILE_PATH} sheet: {FEATURES_SHEET_NAME}\")\n",
        "    try:\n",
        "        # Read df_features from the .xlsx file, specifying the sheet name\n",
        "        df_master = pd.read_excel(FEATURES_FILE_PATH, sheet_name=FEATURES_SHEET_NAME)\n",
        "        # Standardize 'Journey Id' column name to 'Journey id' ---\n",
        "        if 'Journey Id' in df_master.columns:\n",
        "            df_master.rename(columns={'Journey Id': 'Journey id'}, inplace=True)\n",
        "        # Standardize 'Has CV SWS ' column name to 'Has CV SWS' ---\n",
        "        if 'Has CV SWS ' in df_master.columns:\n",
        "            df_master.rename(columns={'Has CV SWS ': 'Has CV SWS'}, inplace=True)\n",
        "        print(f\"Loaded {len(df_master)} rows from JobSeeker sheet.\")\n",
        "        # --- DIAGNOSTIC: Print columns of df_master ---\n",
        "        print(f\"Columns in df_master: {df_master.columns.tolist()}\")\n",
        "        # --- DIAGNOSTIC: Print value counts of 'Status' column in JobSeeker ---\n",
        "        if 'Status' in df_master.columns:\n",
        "            print(f\"Value counts for 'Status' in JobSeeker: \\n{df_master['Status'].value_counts()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR loading features file: {e} !!!---\")\n",
        "        print(\"Please check: 1. File path is correct. 2. Sheet name is correct. 3. 'openpyxl' is installed for .xlsx.\")\n",
        "        return None\n",
        "\n",
        "    # --- 3. Create the Classification Label (The \"Prediction\") ---\n",
        "    # Use 'Status' == 'In work' from the JobSeeker sheet as our \"Hired\" (1) or \"Not Hired\" (0) label\n",
        "    # Fill NaN 'Status' with a placeholder for consistent logic\n",
        "    df_master['Status'] = df_master['Status'].fillna('Unknown')\n",
        "    df_master['TARGET_HIRED'] = np.where(df_master['Status'] == 'In work', 1, 0)\n",
        "\n",
        "    print(f\"Master dataset created with {len(df_master)} rows.\")\n",
        "\n",
        "    # --- 4. Save the new master file ---\n",
        "    df_master.to_csv(MASTER_DATASET_PATH, index=False)\n",
        "    print(f\"Master dataset saved to: {MASTER_DATASET_PATH}\")\n",
        "    return df_master\n",
        "\n",
        "# --- 5. RUN THE FUNCTION ---\n",
        "master_df = create_master_dataset()\n",
        "if master_df is not None:\n",
        "    print(f\"We have {master_df['TARGET_HIRED'].sum()} 'Hired' users to train on (based on JobSeeker 'Status' == 'In work').\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs_qIuufmxhi",
        "outputId": "8ec86aae-ea1e-4fd8-990d-9e785c0b41cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Creating Master Training Dataset (JobSeeker only) ---\n",
            "Loading data from: /content/drive/MyDrive/JDSampleData.xlsx sheet: JobSeeker\n",
            "Loaded 72641 rows from JobSeeker sheet.\n",
            "Columns in df_master: ['Journey id', 'gender', 'Age', 'Marital status', 'JS Town', 'JS Town distrinct', 'JWD NON JWD', 'Branch', 'Branch region', 'Attachment date', 'Stream', 'Days on programme', 'Status', 'Hired status?', 'Job Position', 'gosi Start Date', 'employer', 'Job id', 'Work status', 'Escalation', 'Salary expectations', 'Highest Qualification', 'Highest major', 'Shifts', 'Working environment', 'Skills English', 'Skills MS Word', 'Skills MS Powerpoint', 'Transport available', 'Driving license', 'Job title 1', 'Job title 2', 'Job title 3', 'Job goal 1', 'Job goal 2', 'Job goal 3', 'Years workexperience', 'Months workexperience', 'Communication skills', 'Has CV SWS', 'Job readiness', 'Number of nominations', 'Attendance mandatory workshops', 'ID Interviews scheduled']\n",
            "Value counts for 'Status' in JobSeeker: \n",
            "Status\n",
            "On programme                 52348\n",
            "In work                      19367\n",
            "In work - Pending quality      926\n",
            "Name: count, dtype: int64\n",
            "Master dataset created with 72641 rows.\n",
            "Master dataset saved to: /content/drive/MyDrive/master_training_dataset.csv\n",
            "We have 19367 'Hired' users to train on (based on JobSeeker 'Status' == 'In work').\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5"
      ],
      "metadata": {
        "id": "Oc9We-v6Pj8r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48e7ab7b",
        "outputId": "6f80ca38-a64f-4c2d-f126-a8ff278af8f2"
      },
      "source": [
        "# This cell builds the \"AI Brain\" (XGBoost Model).\n",
        "\n",
        "\n",
        "print(\"--- Phase 2: Building the AI Brain (Robust XGBoost) ---\")\n",
        "\n",
        "# Define the brain file path here, similar to MASTER_DATASET_PATH\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
        "BRAIN_FILE_PATH = f\"{DRIVE_PATH}ai_brain_pipeline.joblib\"\n",
        "\n",
        "if 'master_df' not in locals() or master_df.empty:\n",
        "    print(\"ERROR: 'master_df' not found. Please run Cell 4 first.\")\n",
        "else:\n",
        "    TARGET_COLUMN = 'TARGET_HIRED'\n",
        "\n",
        "    # 1. Define Features (Strict List - No Leakage)\n",
        "    AI_FEATURE_COLUMNS = [\n",
        "        'gender', 'Age', 'Marital status',\n",
        "        'Salary expectations', 'Highest Qualification', 'Highest major',\n",
        "        'Shifts', 'Working environment', 'Skills English', 'Skills MS Word',\n",
        "        'Skills MS Powerpoint', 'Transport available', 'Driving license',\n",
        "        'Years workexperience',\n",
        "        'Months workexperience', 'Communication skills', 'Has CV SWS',\n",
        "        'Number of nominations',\n",
        "        'Job title 1', 'Job title 2', 'Job title 3', 'Job goal 1', 'Job goal 2', 'Job goal 3'\n",
        "    ]\n",
        "\n",
        "    # 2. Standardize Text (Lowercasing)\n",
        "    # We create a copy to avoid warnings\n",
        "    training_df = master_df.copy()\n",
        "\n",
        "    # --- Standardize 'Has CV SWS ' column name to 'Has CV SWS' ---\n",
        "    if 'Has CV SWS ' in training_df.columns:\n",
        "        training_df.rename(columns={'Has CV SWS ': 'Has CV SWS'}, inplace=True)\n",
        "\n",
        "    # Convert all object/string columns to lowercase\n",
        "    # This ensures \"Data Analyst\" matches \"data analyst\"\n",
        "    for col in AI_FEATURE_COLUMNS:\n",
        "        # Check if the column exists before trying to access its dtype or convert\n",
        "        if col in training_df.columns and training_df[col].dtype == 'object':\n",
        "            training_df[col] = training_df[col].astype(str).str.lower()\n",
        "\n",
        "    # 3. Separate Features\n",
        "    numeric_features = training_df[AI_FEATURE_COLUMNS].select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = training_df[AI_FEATURE_COLUMNS].select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "    ordinal_features = ['Skills English', 'Skills MS Word', 'Skills MS Powerpoint', 'Communication skills']\n",
        "    skill_levels = ['n_a', 'beginner', 'good', 'excellent'] # Lowercase now!\n",
        "\n",
        "    categorical_features = [col for col in categorical_features if col not in ordinal_features]\n",
        "\n",
        "    print(f\"Training on {len(AI_FEATURE_COLUMNS)} features.\")\n",
        "\n",
        "    # 4. Build Pipelines\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    ordinal_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='n_a')),\n",
        "        ('encoder', OrdinalEncoder(categories=[skill_levels] * len(ordinal_features), handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "    ])\n",
        "\n",
        "    # Use OneHot with 'ignore' to handle unseen job titles gracefully\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('ord', ordinal_transformer, ordinal_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # 5. Create Regularized XGBoost Pipeline\n",
        "    # Calculate scale_pos_weight to handle class imbalance\n",
        "    neg_count = training_df[TARGET_COLUMN].value_counts()[0]\n",
        "    pos_count = training_df[TARGET_COLUMN].value_counts()[1]\n",
        "    scale_pos_weight_value = neg_count / pos_count\n",
        "\n",
        "    ai_brain_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', xgb.XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False,\n",
        "            scale_pos_weight=scale_pos_weight_value, # Apply the calculated weight\n",
        "            random_state=42,\n",
        "            # --- REGULARIZATION (The \"Fuzzy\") ---\n",
        "            max_depth=6,          # Updated max_depth from 4 to 6\n",
        "            learning_rate=0.05,   # Slower learning is more robust\n",
        "            n_estimators=300,     # Updated n_estimators from 200 to 300\n",
        "            subsample=0.8,        # Use only 80% of data per tree (adds noise)\n",
        "            colsample_bytree=0.9  # Updated colsample_bytree from 0.8 to 0.9\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 6. Train\n",
        "    print(\"\\nSplitting data and training model...\")\n",
        "    X = training_df[AI_FEATURE_COLUMNS]\n",
        "    y = training_df[TARGET_COLUMN]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    ai_brain_pipeline.fit(X_train, y_train)\n",
        "    print(\"...Training complete.\")\n",
        "\n",
        "    # 7. Evaluate & Feature Importance Check\n",
        "    print(\"\\n--- Model Performance ---\")\n",
        "    preds = ai_brain_pipeline.predict(X_test)\n",
        "    probs = ai_brain_pipeline.predict_proba(X_test)[:, 1]\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, preds):.2%}\")\n",
        "    print(f\"Avg Prediction Probability: {probs.mean():.2%}\")\n",
        "\n",
        "    # Check Feature Importance to spot Leakage\n",
        "    print(\"\\n--- TOP 5 DRIVERS OF HIRING (Check for Leakage) ---\")\n",
        "    # Extract feature names\n",
        "    try:\n",
        "        ohe_feature_names = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
        "        all_names = numeric_features + ordinal_features + list(ohe_feature_names)\n",
        "        importances = ai_brain_pipeline.named_steps['classifier'].feature_importances_\n",
        "\n",
        "        feat_imp = pd.DataFrame({'Feature': all_names, 'Importance': importances})\n",
        "        feat_imp = feat_imp.sort_values(by='Importance', ascending=False).head(10)\n",
        "        print(feat_imp)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not print feature importance (minor issue): {e}\")\n",
        "\n",
        "    # 8. Save\n",
        "    joblib.dump(ai_brain_pipeline, BRAIN_FILE_PATH)\n",
        "    print(f\"\\nSUCCESS: Robust AI Brain saved to: {BRAIN_FILE_PATH}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 2: Building the AI Brain (Robust XGBoost) ---\n",
            "Training on 24 features.\n",
            "\n",
            "Splitting data and training model...\n",
            "...Training complete.\n",
            "\n",
            "--- Model Performance ---\n",
            "Accuracy: 75.39%\n",
            "Avg Prediction Probability: 42.93%\n",
            "\n",
            "--- TOP 5 DRIVERS OF HIRING (Check for Leakage) ---\n",
            "                                          Feature  Importance\n",
            "3                           Number of nominations    0.040618\n",
            "144                           Driving license_yes    0.005838\n",
            "145                                 Has CV SWS_no    0.004725\n",
            "146                                Has CV SWS_yes    0.004647\n",
            "11                         Marital status_married    0.004091\n",
            "1765  Job goal 1_saudi schools for boys and girls    0.004076\n",
            "648                        Job title 1_translator    0.003925\n",
            "1740          Job goal 1_gold and jewellery trade    0.003896\n",
            "1807                 Job goal 2_personal services    0.003709\n",
            "139                       Transport available_nan    0.003684\n",
            "\n",
            "SUCCESS: Robust AI Brain saved to: /content/drive/MyDrive/ai_brain_pipeline.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6"
      ],
      "metadata": {
        "id": "qcofwQ2GPlfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell builds the \"Action Plan Generator\" .\n",
        "\n",
        "\n",
        "print(\"--- Phase 3: Building the Action Plan Generator (SHAP) ---\")\n",
        "\n",
        "# --- 1. Load the Saved AI Brain ---\n",
        "# (BRAIN_FILE_PATH and EXPLAINER_FILE_PATH were defined in Cell 3)\n",
        "\n",
        "EXPLAINER_FILE_PATH = f\"{DRIVE_PATH}shap_explainer.joblib\"\n",
        "\n",
        "try:\n",
        "    ai_brain_pipeline = joblib.load(BRAIN_FILE_PATH)\n",
        "    print(f\"Successfully loaded AI Brain from: {BRAIN_FILE_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR loading 'ai_brain_pipeline.joblib': {e} !!!---\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Separate Pipeline Components ---\n",
        "preprocessor = ai_brain_pipeline.named_steps['preprocessor']\n",
        "model = ai_brain_pipeline.named_steps['classifier']\n",
        "\n",
        "#\n",
        "# We ask the preprocessor for its *actual* output names\n",
        "print(\"Getting feature names directly from the preprocessor...\")\n",
        "try:\n",
        "    all_transformed_feature_names = preprocessor.get_feature_names_out().tolist()\n",
        "    print(f\"Successfully got {len(all_transformed_feature_names)} feature names.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting feature names: {e}.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. Transform Training Data  ---\n",
        "print(\"Transforming training data for SHAP explainer...\")\n",
        "\n",
        "X = master_df[AI_FEATURE_COLUMNS]\n",
        "y = master_df[TARGET_COLUMN]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "print(\"...Data transformed successfully (dense array).\")\n",
        "\n",
        "# --- 5. Build the SHAP Explainer ---\n",
        "print(\"Building SHAP TreeExplainer...\")\n",
        "#\n",
        "explainer = shap.TreeExplainer(model, X_train_transformed)\n",
        "print(\"...Explainer built successfully.\")\n",
        "\n",
        "# --- 6. Save the SHAP Explainer ---\n",
        "joblib.dump(explainer, EXPLAINER_FILE_PATH)\n",
        "print(f\"SUCCESS: SHAP Explainer saved to: {EXPLAINER_FILE_PATH}\")\n",
        "\n",
        "# --- 7. Define Test Function ---\n",
        "def generate_action_plan_test(new_user_data_df):\n",
        "    prediction_proba = ai_brain_pipeline.predict_proba(new_user_data_df)[0]\n",
        "    hire_probability = prediction_proba[1]\n",
        "    prediction_raw = ai_brain_pipeline.predict(new_user_data_df)[0]\n",
        "\n",
        "    # We DON'T call .toarray() here\n",
        "    user_transformed = preprocessor.transform(new_user_data_df)\n",
        "    shap_values = explainer.shap_values(user_transformed)\n",
        "\n",
        "    # This will now work\n",
        "    df_shap = pd.DataFrame(shap_values, columns=all_transformed_feature_names).iloc[0].T\n",
        "    df_shap = df_shap.reset_index(); df_shap.columns = ['Feature', 'SHAP_Value']\n",
        "    df_shap['abs_impact'] = df_shap['SHAP_Value'].abs()\n",
        "    df_shap = df_shap.sort_values(by='abs_impact', ascending=False)\n",
        "\n",
        "    top_factors = df_shap.head(5)\n",
        "\n",
        "    action_plan = \"--- (TEST) Recommended Action Plan ---\\n\"\n",
        "    for _, row in top_factors.iterrows():\n",
        "        sign = \"[+]\" if row['SHAP_Value'] > 0 else \"[-]\"\n",
        "        action_plan += f\"  {sign} {row['Feature']} (Impact: {row['SHAP_Value']:.2f})\\n\"\n",
        "    action_plan += f\"\\n  Prediction: {'Hired' if prediction_raw == 1 else 'Not Hired'} ({hire_probability:.1%})\"\n",
        "    return action_plan\n",
        "\n",
        "# --- 8. Test the Explainer ---\n",
        "print(\"\\n--- TESTING THE ACTION PLAN GENERATOR ---\")\n",
        "# We use X_test, which we just created in this cell\n",
        "sample_user_df = X_test.iloc[0:1]\n",
        "true_label = y_test.iloc[0]\n",
        "print(f\"Generating plan for a sample user. (True Label: {'Hired' if true_label == 1 else 'Not Hired'})...\")\n",
        "print(generate_action_plan_test(sample_user_df))\n",
        "print(\"---------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J34l0WQlu-22",
        "outputId": "f81c559d-960a-4296-93d5-baabe92c0dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 3: Building the Action Plan Generator (SHAP) ---\n",
            "Successfully loaded AI Brain from: /content/drive/MyDrive/ai_brain_pipeline.joblib\n",
            "Getting feature names directly from the preprocessor...\n",
            "Successfully got 1882 feature names.\n",
            "Transforming training data for SHAP explainer...\n",
            "...Data transformed successfully (dense array).\n",
            "Building SHAP TreeExplainer...\n",
            "...Explainer built successfully.\n",
            "SUCCESS: SHAP Explainer saved to: /content/drive/MyDrive/shap_explainer.joblib\n",
            "\n",
            "--- TESTING THE ACTION PLAN GENERATOR ---\n",
            "Generating plan for a sample user. (True Label: Hired)...\n",
            "--- (TEST) Recommended Action Plan ---\n",
            "  [+] num__Number of nominations (Impact: 1.04)\n",
            "  [+] num__Age (Impact: 0.08)\n",
            "  [-] num__Years workexperience (Impact: -0.04)\n",
            "  [+] cat__Job title 3_tele-customer service (Impact: 0.00)\n",
            "  [+] cat__Job title 2_tele-customer service (Impact: 0.00)\n",
            "\n",
            "  Prediction: Hired (58.3%)\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7"
      ],
      "metadata": {
        "id": "A1f1Wm94Pnkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import json\n",
        "import httpx\n",
        "import asyncio\n",
        "from google.colab import userdata\n",
        "import matplotlib.pyplot as plt # Moved here\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "import os\n",
        "\n",
        "# --- Install pypdf for resume parsing ---\n",
        "try:\n",
        "    import pypdf\n",
        "except ImportError:\n",
        "    os.system('pip install -q pypdf')\n",
        "    import pypdf\n",
        "\n",
        "print(\"--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\")\n",
        "\n",
        "# --- 1. Load API Key from Colab Secrets ---\n",
        "try:\n",
        "    MY_GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    if not MY_GOOGLE_API_KEY:\n",
        "        raise ValueError(\"API Key is empty or not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR: API KEY NOT FOUND: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Global Variables ---\n",
        "ai_models = {\"pipeline\": None, \"explainer\": None, \"feature_names\": None}\n",
        "global_user_df = pd.DataFrame()\n",
        "processed_new_candidates_df = pd.DataFrame() # Initialize global for new candidate analysis\n",
        "\n",
        "# Define paths explicitly to avoid scope issues\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
        "BRAIN_FILE_PATH = f\"{DRIVE_PATH}ai_brain_pipeline.joblib\"\n",
        "EXPLAINER_FILE_PATH = f\"{DRIVE_PATH}shap_explainer.joblib\" # explainer filename\n",
        "MASTER_DATASET_PATH = f\"{DRIVE_PATH}master_training_dataset.csv\"\n",
        "NEW_SUBMISSIONS_FILE_PATH = f\"{DRIVE_PATH}new_submissions.csv\"\n",
        "\n",
        "# This list MUST match the list in Cell 5\n",
        "ALL_FEATURE_COLUMNS = [\n",
        "    'gender', 'Age', 'Marital status',\n",
        "    'Salary expectations', 'Highest Qualification', 'Highest major',\n",
        "    'Shifts', 'Working environment', 'Skills English', 'Skills MS Word',\n",
        "    'Skills MS Powerpoint', 'Transport available', 'Driving license',\n",
        "    'Years workexperience',\n",
        "    'Months workexperience', 'Communication skills', 'Has CV SWS',\n",
        "    'Number of nominations',\n",
        "    'Job title 1', 'Job title 2', 'Job title 3', 'Job goal 1', 'Job goal 2', 'Job goal 3'\n",
        "]\n",
        "\n",
        "# --- 3. Define Dropdown Choices for the UI ---\n",
        "GENDER_CHOICES = ['Male', 'Female', 'Other', 'Prefer not to say']\n",
        "MARITAL_CHOICES = ['Single', 'Married', 'Divorced', 'Widowed', 'Other']\n",
        "SALARY_CHOICES = ['< 3,000SAR', '3,000 - 5,000SAR', '5,000 - 7,000SAR', '> 7,000SAR']\n",
        "QUALIFICATION_CHOICES = ['High School', 'Diploma', 'Bachelor', 'Masters', 'Doctorate']\n",
        "SHIFTS_CHOICES = ['No preference', 'Straight shifts', 'Rotating shifts']\n",
        "ENVIRONMENT_CHOICES = ['Mixed', 'Flexible', 'On-site', 'Remote']\n",
        "SKILL_LEVEL_CHOICES = ['N_A', 'Beginner', 'Good', 'Excellent']\n",
        "YES_NO_CHOICES = ['Yes', 'No']\n",
        "COMMUNICATION_CHOICES = ['Beginner', 'Good', 'Excellent']\n",
        "\n",
        "\n",
        "# --- 4. Function to Load XGBoost AI Brain (Runs Once) ---\n",
        "def on_app_load():\n",
        "    global ai_models, global_user_df\n",
        "    print(\"Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\")\n",
        "    try:\n",
        "        ai_models[\"pipeline\"] = joblib.load(BRAIN_FILE_PATH)\n",
        "        ai_models[\"explainer\"] = joblib.load(EXPLAINER_FILE_PATH)\n",
        "        preprocessor = ai_models[\"pipeline\"].named_steps['preprocessor']\n",
        "        ai_models[\"feature_names\"] = preprocessor.get_feature_names_out().tolist()\n",
        "        print(\"...XGBoost Models loaded successfully.\")\n",
        "\n",
        "        # Load Global Data\n",
        "        if os.path.exists(MASTER_DATASET_PATH):\n",
        "            global_user_df = pd.read_csv(MASTER_DATASET_PATH)\n",
        "            print(f\"...Loaded {len(global_user_df)} users for Global Dashboard.\")\n",
        "        else:\n",
        "            print(\"Warning: Master dataset not found.\")\n",
        "\n",
        "        return \"System Ready. AI Models and Global Data loaded.\"\n",
        "    except Exception as e:\n",
        "        return f\"CRITICAL ERROR: Could not load AI modules or data. {e}\"\n",
        "\n",
        "# --- 5. Helper Functions ---\n",
        "def format_feature_name_for_llm(feature_name):\n",
        "    name = feature_name.replace(\"cat__\", \"\").replace(\"ord__\", \"\").replace(\"num__\", \"\")\n",
        "    parts = name.split('_', 1)\n",
        "    if len(parts) == 2:\n",
        "        if \"Skills\" in parts[0]: return parts[0].replace(\"Skills \", \"\")\n",
        "        if \"Job goal\" in parts[0]: return \"Job Goal Setting\"\n",
        "        if \"Job title\" in parts[0]: return \"Job Title Alignment\"\n",
        "        return parts[0]\n",
        "    return name\n",
        "\n",
        "def run_xgb_prediction_and_get_report(profile_dict):\n",
        "    print(\"XGBoost Brain: Preparing prediction...\")\n",
        "    try:\n",
        "        if ai_models[\"pipeline\"] is None: return \"Error: XGBoost model not loaded.\", \"Unknown\", 0.0\n",
        "\n",
        "        # DEBUG: Print input to verify it's not empty\n",
        "        # print(f\"DEBUG Input Profile: {profile_dict}\")\n",
        "\n",
        "        new_user_df = pd.DataFrame([profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "\n",
        "        # Ensure numeric types are correct\n",
        "        numeric_cols = ['Age', 'Years workexperience', 'Months workexperience', 'Number of nominations']\n",
        "        for col in numeric_cols:\n",
        "            new_user_df[col] = pd.to_numeric(new_user_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "        pipeline = ai_models[\"pipeline\"]\n",
        "        explainer = ai_models[\"explainer\"]\n",
        "        feature_names = ai_models[\"feature_names\"]\n",
        "\n",
        "        # Prediction\n",
        "        prediction_proba = pipeline.predict_proba(new_user_df)[0]\n",
        "        hire_probability = prediction_proba[1]\n",
        "        print(f\"DEBUG Prediction: {hire_probability}\")\n",
        "\n",
        "        # Explanation\n",
        "        user_transformed = pipeline.named_steps['preprocessor'].transform(new_user_df)\n",
        "        shap_values = explainer.shap_values(user_transformed)\n",
        "        df_shap = pd.DataFrame(shap_values, columns=feature_names).iloc[0].T\n",
        "        df_shap = df_shap.reset_index(); df_shap.columns = ['Feature', 'SHAP_Value']\n",
        "        df_shap['abs_impact'] = df_shap['SHAP_Value'].abs()\n",
        "\n",
        "        df_negatives = df_shap[df_shap['SHAP_Value'] < 0].sort_values(by='SHAP_Value', ascending=True)\n",
        "        top_weakness_topic = \"General Profile Improvement\"\n",
        "        if not df_negatives.empty:\n",
        "            top_weakness_topic = format_feature_name_for_llm(df_negatives.iloc[0]['Feature'])\n",
        "\n",
        "        report = f\"STATISTICAL ANALYSIS REPORT:\\nPredicted Hire Probability: {hire_probability:.1%}\\nTop 5 Factors:\\n\"\n",
        "        for _, row in df_shap.sort_values(by='abs_impact', ascending=False).head(5).iterrows():\n",
        "            sign = \"POSITIVE\" if row['SHAP_Value'] > 0 else \"NEGATIVE\"\n",
        "            report += f\"  - Factor: {row['Feature']}, Impact: {sign}\\n\"\n",
        "\n",
        "        return report, top_weakness_topic, hire_probability\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction Error: {e}\")\n",
        "        return f\"Error during XGBoost prediction: {e}\", \"Unknown\", 0.0\n",
        "\n",
        "# --- 6. Resume Parsing Logic ---\n",
        "async def parse_resume(file_obj, *current_inputs):\n",
        "    if file_obj is None: return list(current_inputs)\n",
        "    try:\n",
        "        pdf_reader = pypdf.PdfReader(file_obj.name)\n",
        "        resume_text = \"\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "        prompt = f\"\"\"\n",
        "        Extract data from RESUME TEXT to JSON.\n",
        "        RESUME TEXT: {resume_text[:10000]}\n",
        "        REQUIRED KEYS: {ALL_FEATURE_COLUMNS}\n",
        "        Use \"Unknown\" or 0 if missing.\n",
        "        \"\"\"\n",
        "        json_str = await call_gemini_api(prompt, json_mode=True)\n",
        "        parsed_data = json.loads(json_str)\n",
        "        updates = []\n",
        "        for col in ALL_FEATURE_COLUMNS:\n",
        "            val = parsed_data.get(col, None)\n",
        "            updates.append(gr.update(value=val) if val is not None else gr.update())\n",
        "        return updates\n",
        "    except Exception as e:\n",
        "        print(f\"Resume parsing error: {e}\")\n",
        "        return list(current_inputs)\n",
        "\n",
        "# --- 7. Chart Generation Functions ---\n",
        "def generate_individual_charts(profile_dict):\n",
        "    skills = {k: profile_dict.get(f'Skills {k}', 'N_A') for k in ['English', 'MS Word', 'MS Powerpoint']}\n",
        "    skills['Communication'] = profile_dict.get('Communication skills', 'N_A')\n",
        "    level_map = {'N_A': 0, 'Beginner': 1, 'Good': 2, 'Excellent': 3}\n",
        "    values = [level_map.get(v, 0) for v in skills.values()]\n",
        "    labels = list(skills.keys())\n",
        "    N = len(labels); angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
        "    values += values[:1]; angles += angles[:1]\n",
        "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
        "    ax.fill(angles, values, color='blue', alpha=0.25); ax.plot(angles, values, color='blue', linewidth=2)\n",
        "    ax.set_xticks(angles[:-1]); ax.set_xticklabels(labels); plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_hiring_rate_by_category(df, category_col, ax):\n",
        "    # Fill NaN values in the category_col with 'Unknown' for consistent grouping\n",
        "    df_plot = df.copy()\n",
        "    # Convert to object (string) type first, then fillna, to ensure 'Unknown' is accepted\n",
        "    df_plot[category_col] = df_plot[category_col].astype(str).fillna('Unknown')\n",
        "\n",
        "    # Calculate hiring rate\n",
        "    hiring_rates = df_plot.groupby(category_col)['TARGET_HIRED'].mean().sort_index()\n",
        "\n",
        "    # Plot bar chart\n",
        "    hiring_rates.plot(kind='bar', ax=ax, color='skyblue')\n",
        "    ax.set_title(f'Hiring Rate by {category_col}')\n",
        "    ax.set_ylabel('Hiring Rate')\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.tick_params(axis='x', labelrotation=45)\n",
        "    ax.set_xlabel('')\n",
        "\n",
        "def generate_global_dashboard():\n",
        "    global global_user_df\n",
        "    if global_user_df.empty or 'TARGET_HIRED' not in global_user_df.columns:\n",
        "        fig, ax = plt.subplots(); ax.text(0.5, 0.5, \"Global user data not loaded or TARGET_HIRED missing.\", ha='center'); return fig\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Plot 1: Overall Hiring Success Rate\n",
        "    overall_hiring_rate = global_user_df['TARGET_HIRED'].mean()\n",
        "    axes[0].bar(['Overall'], [overall_hiring_rate], color='lightcoral')\n",
        "    axes[0].set_title('Overall Hiring Success Rate')\n",
        "    axes[0].set_ylabel('Hiring Rate')\n",
        "    axes[0].set_ylim(0, 1)\n",
        "    axes[0].text(0, overall_hiring_rate + 0.05, f'{overall_hiring_rate:.2%}', ha='center', va='bottom')\n",
        "\n",
        "    # Plot 2: Hiring Rate by Gender\n",
        "    plot_hiring_rate_by_category(global_user_df, 'gender', axes[1])\n",
        "\n",
        "    # Plot 3: Hiring Rate by Highest Qualification\n",
        "    plot_hiring_rate_by_category(global_user_df, 'Highest Qualification', axes[2])\n",
        "\n",
        "    # Plot 4: Hiring Rate by Years Work Experience\n",
        "    df_temp = global_user_df.copy()\n",
        "    df_temp['Years workexperience'] = pd.to_numeric(df_temp['Years workexperience'], errors='coerce').fillna(0)\n",
        "    bins = [-1, 0, 1, 5, 10, np.inf] # Including -1 to catch 0 years\n",
        "    labels = ['0', '<1', '1-5', '6-10', '>10']\n",
        "    df_temp['binned_years_exp'] = pd.cut(df_temp['Years workexperience'], bins=bins, labels=labels, right=False)\n",
        "    plot_hiring_rate_by_category(df_temp, 'binned_years_exp', axes[3])\n",
        "\n",
        "    # Plot 5: Hiring Rate by Has CV SWS\n",
        "    plot_hiring_rate_by_category(global_user_df, 'Has CV SWS', axes[4])\n",
        "\n",
        "    # Plot 6: Hiring Rate by Number of Nominations\n",
        "    df_temp = global_user_df.copy()\n",
        "    df_temp['Number of nominations'] = pd.to_numeric(df_temp['Number of nominations'], errors='coerce').fillna(0)\n",
        "    bins = [-1, 0, 1, 2, 5, np.inf] # Including -1 to catch 0 nominations\n",
        "    labels = ['0', '1', '2', '3-5', '>5']\n",
        "    df_temp['binned_nominations'] = pd.cut(df_temp['Number of nominations'], bins=bins, labels=labels, right=False)\n",
        "    plot_hiring_rate_by_category(df_temp, 'binned_nominations', axes[5])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# --- 8. Submit Function ---\n",
        "async def process_new_user_submission(*args):\n",
        "    if ai_models[\"pipeline\"] is None: return \"ERROR: AI Models not loaded.\", \"Error\", None, \"Error\", None, None, None\n",
        "\n",
        "    new_profile_dict = dict(zip(ALL_FEATURE_COLUMNS, args))\n",
        "\n",
        "    # Save data\n",
        "    try:\n",
        "        save_data = pd.DataFrame([new_profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "        save_data['Journey id'] = f\"WEB-{int(datetime.datetime.now().timestamp())}\"\n",
        "        save_data['Attachment date'] = datetime.date.today().isoformat()\n",
        "        save_data['Stream'] = 'Online Submission'; save_data['Work status'] = 'On programme'\n",
        "        file_exists = os.path.isfile(NEW_SUBMISSIONS_FILE_PATH)\n",
        "        save_data.to_csv(NEW_SUBMISSIONS_FILE_PATH, mode='a', header=not file_exists, index=False)\n",
        "        print(\"Profile saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- ERROR Saving: {e} ---\")\n",
        "\n",
        "    statistical_report, top_weakness, hire_probability = run_xgb_prediction_and_get_report(new_profile_dict)\n",
        "    individual_skill_chart = generate_individual_charts(new_profile_dict)\n",
        "\n",
        "    years = pd.to_numeric(new_profile_dict.get('Years workexperience', 0), errors='coerce')\n",
        "    months = pd.to_numeric(new_profile_dict.get('Months workexperience', 0), errors='coerce')\n",
        "    total_years = years + (months / 12)\n",
        "    level = 'Junior' if total_years <= 2 else ('Mid-level' if total_years <= 5 else 'Senior')\n",
        "    categorization_text = f\"**Level:** {level}\\n**Qualification:** {new_profile_dict.get('Highest Qualification', 'N/A')}\\n**Top Job Goal:** {new_profile_dict.get('Job goal 1', 'N/A')}\"\n",
        "\n",
        "    # RAG Action Plan\n",
        "    job_goal = new_profile_dict.get('Job goal 1', 'N_A'); major = new_profile_dict.get('Highest major', 'N_A')\n",
        "    system_prompt = f\"\"\"Act as an AI Career Counselor.\n",
        "    Profile: {json.dumps(new_profile_dict, indent=2)}\n",
        "    Stats: {statistical_report}\n",
        "    Weakness: {top_weakness}\n",
        "    Goal: {job_goal}\n",
        "\n",
        "    Task: Use Google Search to find courses/meetups for {job_goal} and {top_weakness}.\n",
        "    Write a comprehensive career plan with:\n",
        "    1. Analysis of hire probability.\n",
        "    2. Recommended internal workshops (from list: CV Writing, Interview Skills).\n",
        "    3. 2-3 External Courses (MUST use Google Search).\n",
        "    4. Meetups/Networking events.\n",
        "    \"\"\"\n",
        "\n",
        "    final_action_plan = await call_gemini_api(system_prompt, tools=[{\"google_search\": {}}], json_mode=False)\n",
        "\n",
        "    # Success Stories\n",
        "    hired_df = global_user_df[global_user_df['TARGET_HIRED'] == 1].copy() if not global_user_df.empty and 'TARGET_HIRED' in global_user_df.columns else pd.DataFrame()\n",
        "    stories = \"### \\u2001\\u2001\\u2001\\u2001 \\u2001Success Stories\\nNo exact matches found.\"\n",
        "    if not hired_df.empty:\n",
        "        # Removed 'Job goal 1' from here as well, if it's not a feature in the model\n",
        "        stories += \"Based on available data, no exact matches for success stories can be generated at this moment.\"\n",
        "\n",
        "    updated_dashboard = generate_global_dashboard()\n",
        "\n",
        "    return (\n",
        "        f\"{hire_probability:.1%}\", categorization_text, individual_skill_chart, final_action_plan, final_action_plan, updated_dashboard, stories\n",
        "    )\n",
        "\n",
        "# --- 9. Follow-up Chat Function ---\n",
        "async def call_gemini_follow_up_chat(user_message: str, chat_history: list, report_context: str):\n",
        "    chat_history.append([user_message, None])\n",
        "    if not report_context:\n",
        "        chat_history[-1][1] = \"Please generate a report first.\"\n",
        "        return \"\", chat_history\n",
        "    system_prompt = f\"Context: {report_context}. User: '{user_message}'. Answer using context and Google Search.\"\n",
        "    try:\n",
        "        bot_response = await call_gemini_api(system_prompt, tools=[{\"google_search\": {}}]) # Added tools argument here\n",
        "    except Exception as e:\n",
        "        bot_response = f\"Error generating question: {e}. Please try again.\"\n",
        "\n",
        "    chat_history[-1][1] = bot_response\n",
        "    return \"\", chat_history\n",
        "\n",
        "# --- 10. Generic Gemini API Caller ---\n",
        "async def call_gemini_api(prompt, tools=None, json_mode=False):\n",
        "    apiKey = MY_GOOGLE_API_KEY\n",
        "    apiUrl = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={apiKey}\"\n",
        "    generation_config = {\"responseMimeType\": \"application/json\"} if json_mode else {\"responseMimeType\": \"text/plain\"}\n",
        "    payload = {\"contents\": ([{\"parts\": [{\"text\": prompt}]}])}\n",
        "    if tools: payload[\"tools\"] = tools\n",
        "    payload[\"generationConfig\"] = generation_config # Moved generationConfig here\n",
        "\n",
        "    transport = httpx.AsyncHTTPTransport(retries=3)\n",
        "    async with httpx.AsyncClient(transport=transport) as client:\n",
        "        response = await client.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'}, timeout=90.0)\n",
        "        if response.status_code != 200: raise Exception(f\"API Error: {response.text}\")\n",
        "        result = response.json()\n",
        "        return result['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "# --- Mock Interview Placeholder Functions (added to resolve NameError) ---\n",
        "async def start_mock_interview(role: str, chat_history: list):\n",
        "    initial_message = f\"Hello! I'm your AI Interviewer. We'll be focusing on a {role} role. Let's start with your experience. Tell me about a time you faced a challenge in a previous role and how you overcame it.\"\n",
        "    chat_history.append((None, initial_message))\n",
        "    return chat_history\n",
        "\n",
        "async def continue_mock_interview(user_response: str, chat_history: list, role: str):\n",
        "    chat_history.append((user_response, None))\n",
        "\n",
        "    # Generate a new question using Gemini API\n",
        "    conversation_history_text = \"\\n\".join([f\"{'Interviewer' if i % 2 == 0 else 'Candidate'}: {msg[1] if msg[1] else msg[0]}\" for i, msg in enumerate(chat_history)])\n",
        "\n",
        "    prompt = f\"\"\"You are an AI interviewer for a {role} role. The candidate just responded to your last question.\n",
        "    Your previous questions and the candidate's responses are below. Generate ONE *unique*, relevant, and challenging follow-up interview question.\n",
        "    Ensure this question has not been asked before in the conversation history.\n",
        "    Do NOT ask for personal information. Do NOT start with 'Okay' or similar filler words. Keep the question concise and professional.\n",
        "\n",
        "    Conversation History:\n",
        "    {conversation_history_text}\n",
        "\n",
        "    Candidate's last response: {user_response}\n",
        "\n",
        "    Your next question:\"\"\"\n",
        "    try:\n",
        "        bot_response = await call_gemini_api(prompt)\n",
        "    except Exception as e:\n",
        "        bot_response = f\"Error generating question: {e}. Please try again.\"\n",
        "\n",
        "    chat_history[-1] = (user_response, bot_response)\n",
        "    return \"\", chat_history\n",
        "\n",
        "# --- New function to autofill with sample data ---\n",
        "def autofill_sample_data():\n",
        "    return [\n",
        "        'Male', # gender\n",
        "        30, # Age\n",
        "        'Married', # Marital status\n",
        "        '5,000 - 7,000SAR', # Salary expectations\n",
        "        'Bachelor', # Highest Qualification\n",
        "        'Electrical Engineering', # Highest major\n",
        "        'No preference', # Shifts\n",
        "        'Mixed', # Working environment\n",
        "        'Excellent', # Skills English\n",
        "        'Good', # Skills MS Word\n",
        "        'Good', # Skills MS Powerpoint\n",
        "        'Yes', # Transport available\n",
        "        'Yes', # Driving license\n",
        "        7, # Years workexperience\n",
        "        6, # Months workexperience\n",
        "        'Excellent', # Communication skills\n",
        "        'Yes', # Has CV SWS\n",
        "        5, # Number of nominations\n",
        "        'Software Engineer', # Job title 1\n",
        "        'Data Analyst', # Job title 2\n",
        "        'Project Manager', # Job title 3\n",
        "        'Software Development', # Job goal 1\n",
        "        'Data Science', # Job goal 2\n",
        "        'Project Management' # Job goal 3\n",
        "    ]\n",
        "\n",
        "# --- New Candidate Analysis Function ---\n",
        "def get_new_candidate_analysis_data():\n",
        "    global processed_new_candidates_df\n",
        "    print(\"Running new candidate analysis...\")\n",
        "    if ai_models[\"pipeline\"] is None:\n",
        "        return pd.DataFrame(), None, \"ERROR: AI Models not loaded.\"\n",
        "\n",
        "    if not os.path.exists(NEW_SUBMISSIONS_FILE_PATH):\n",
        "        return pd.DataFrame(), None, \"No new submissions found to analyze.\"\n",
        "\n",
        "    try:\n",
        "        new_submissions_df = pd.read_csv(NEW_SUBMISSIONS_FILE_PATH)\n",
        "        print(f\"Loaded {len(new_submissions_df)} new submissions.\")\n",
        "\n",
        "        # : Align new_submissions_df columns with ALL_FEATURE_COLUMNS\n",
        "        # Identify missing columns and add them\n",
        "        missing_cols = set(ALL_FEATURE_COLUMNS) - set(new_submissions_df.columns)\n",
        "        for c in missing_cols:\n",
        "            # Add missing columns with NaN values. These will be handled by the preprocessor's imputer.\n",
        "            new_submissions_df[c] = np.nan\n",
        "\n",
        "        # Identify extra columns and drop them. Exclude columns that might be added by the submission saving process.\n",
        "        extra_cols = set(new_submissions_df.columns) - set(ALL_FEATURE_COLUMNS) - {'Journey id', 'Attachment date', 'Stream', 'Work status', 'Predicted_Hire_Probability', 'Predicted_Hired'}\n",
        "        if extra_cols:\n",
        "            print(f\"Dropping extra columns from new_submissions_df: {extra_cols}\")\n",
        "            new_submissions_df = new_submissions_df.drop(columns=list(extra_cols))\n",
        "\n",
        "        # Ensure the order of columns matches ALL_FEATURE_COLUMNS for consistency when processing\n",
        "        new_submissions_df = new_submissions_df[list(new_submissions_df.columns.intersection(ALL_FEATURE_COLUMNS)) + list(new_submissions_df.columns.difference(ALL_FEATURE_COLUMNS))]\n",
        "\n",
        "        # Ensure numeric types are correct before prediction\n",
        "        numeric_cols = ['Age', 'Years workexperience', 'Months workexperience', 'Number of nominations']\n",
        "        for col in numeric_cols:\n",
        "            # Use .loc to avoid SettingWithCopyWarning if new_submissions_df is a slice\n",
        "            new_submissions_df.loc[:, col] = pd.to_numeric(new_submissions_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "        # Standardize categorical columns to lowercase for consistency with training data\n",
        "        for col in ALL_FEATURE_COLUMNS:\n",
        "            if col in new_submissions_df.columns and new_submissions_df[col].dtype == 'object':\n",
        "                # Use .loc to avoid SettingWithCopyWarning\n",
        "                new_submissions_df.loc[:, col] = new_submissions_df[col].astype(str).str.lower()\n",
        "\n",
        "        # Predict probabilities and assign a 'TARGET_HIRED' based on a threshold (e.g., 0.5)\n",
        "        # Ensure only ALL_FEATURE_COLUMNS are used for prediction\n",
        "        probabilities = ai_models[\"pipeline\"].predict_proba(new_submissions_df[ALL_FEATURE_COLUMNS])[:, 1]\n",
        "        new_submissions_df['Predicted_Hire_Probability'] = probabilities\n",
        "        new_submissions_df['Predicted_Hired'] = (probabilities >= 0.5).astype(int) # Example threshold\n",
        "\n",
        "        processed_new_candidates_df = new_submissions_df.copy()\n",
        "\n",
        "        # Generate summary\n",
        "        hired_count = processed_new_candidates_df['Predicted_Hired'].sum()\n",
        "        total_candidates = len(processed_new_candidates_df)\n",
        "        hiring_rate = (hired_count / total_candidates) * 100 if total_candidates > 0 else 0\n",
        "        summary = f\"### New Candidate Analysis Summary\\n\\nTotal Candidates: {total_candidates}\\nPredicted Hired: {hired_count} ({hiring_rate:.2f}%)\\n\\nAverage Predicted Probability: {processed_new_candidates_df['Predicted_Hire_Probability'].mean():.2%}\"\n",
        "\n",
        "        # Generate plot (e.g., distribution of predicted probabilities)\n",
        "        fig_prob = px.histogram(processed_new_candidates_df, x='Predicted_Hire_Probability', nbins=20,\n",
        "                                title='Distribution of Predicted Hiring Probabilities',\n",
        "                                labels={'Predicted_Hire_Probability': 'Probability of Being Hired'})\n",
        "        fig_prob.update_layout(bargap=0.1)\n",
        "\n",
        "        return processed_new_candidates_df, fig_prob, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during new candidate analysis: {e}\")\n",
        "        return pd.DataFrame(), None, f\"Error during analysis: {e}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Iy_4NJ-ou_Y",
        "outputId": "f6438975-5b21-4e2e-fe60-2d088322f4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8"
      ],
      "metadata": {
        "id": "JhkC_YkFPpJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Launching Ultimate App...\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# AI-Powered Predictive Hiring & Action Intelligence System\")\n",
        "\n",
        "    # State variables\n",
        "    report_context_state = gr.State(value=\"\")\n",
        "    chat_history_state = gr.State(value=[])\n",
        "\n",
        "    # --- TOP LEVEL TABS ---\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # --- PAGE 1: PROFILE & ACTION PLANNING ---\n",
        "        with gr.TabItem(\" Profile & Action Planning\"):\n",
        "            with gr.Row():\n",
        "                # --- LEFT COLUMN: INPUT FORM ---\n",
        "                with gr.Column(scale=3):\n",
        "                    gr.Markdown(\"### 1. Profile Input\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        resume_file = gr.File(label=\" Auto-Fill Profile from Resume (PDF)\", file_types=[\".pdf\"], scale=3)\n",
        "                        parse_btn = gr.Button(\"Parse\", scale=1)\n",
        "\n",
        "                    autofill_btn = gr.Button(\" Autofill Sample Data\") # New Autofill Button\n",
        "\n",
        "                    # FORM INPUTS MAPPING\n",
        "                    inputs_map = {}\n",
        "\n",
        "                    with gr.Accordion(\"Personal Info\", open=True):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['gender'] = gr.Dropdown(label=\"Gender\", choices=GENDER_CHOICES, value='Female')\n",
        "                            inputs_map['Age'] = gr.Number(label=\"Age\", value=25)\n",
        "                        inputs_map['Marital status'] = gr.Dropdown(label=\"Marital Status\", choices=MARITAL_CHOICES, value='Single')\n",
        "\n",
        "                    # Removed 'JS Town' and 'JS Town distrinct' inputs as they are no longer model features\n",
        "\n",
        "                    with gr.Accordion(\"Education & Salary\", open=False):\n",
        "                        inputs_map['Salary expectations'] = gr.Dropdown(label=\"Salary Expectations\", choices=SALARY_CHOICES, value='3,000 - 5,000SAR')\n",
        "                        inputs_map['Highest Qualification'] = gr.Dropdown(label=\"Highest Qualification\", choices=QUALIFICATION_CHOICES, value='Bachelor')\n",
        "                        inputs_map['Highest major'] = gr.Textbox(label=\"Highest Major\", value='Computer Science')\n",
        "\n",
        "                    with gr.Accordion(\"Work Preferences\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Shifts'] = gr.Dropdown(label=\"Shifts\", choices=SHIFTS_CHOICES, value='No preference')\n",
        "                            inputs_map['Working environment'] = gr.Dropdown(label=\"Environment\", choices=ENVIRONMENT_CHOICES, value='Mixed')\n",
        "\n",
        "                    with gr.Accordion(\"Skills\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Skills English'] = gr.Dropdown(label=\"English\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                            inputs_map['Skills MS Word'] = gr.Dropdown(label=\"Word\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                            # Removed 'Skills MS Excel' as it's not in the model training features\n",
        "                            inputs_map['Skills MS Powerpoint'] = gr.Dropdown(label=\"PPT\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Communication skills'] = gr.Dropdown(label=\"Communication\", choices=COMMUNICATION_CHOICES, value='Good')\n",
        "\n",
        "                    with gr.Accordion(\"Logistics\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Transport available'] = gr.Radio(label=\"Transport?\", choices=YES_NO_CHOICES, value='No')\n",
        "                            inputs_map['Driving license'] = gr.Radio(label=\"License?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        inputs_map['Has CV SWS'] = gr.Radio(label=\"CV?\", choices=YES_NO_CHOICES, value='No') # Corrected column name\n",
        "\n",
        "                    with gr.Accordion(\"Target Roles & Goals\", open=False):\n",
        "                        inputs_map['Job title 1'] = gr.Textbox(label=\"Job Title 1\", value=\"Software Engineer\")\n",
        "                        inputs_map['Job title 2'] = gr.Textbox(label=\"Job Title 2\", value=\"Data Analyst\")\n",
        "                        inputs_map['Job title 3'] = gr.Textbox(label=\"Job Title 3\", value=\"Project Manager\")\n",
        "                        inputs_map['Job goal 1'] = gr.Textbox(label=\"Job Goal 1\", value=\"Software Development\")\n",
        "                        inputs_map['Job goal 2'] = gr.Textbox(label=\"Job Goal 2\", value=\"Data Science\")\n",
        "                        inputs_map['Job goal 3'] = gr.Textbox(label=\"Job Goal 3\", value=\"Project Management\")\n",
        "\n",
        "                    with gr.Accordion(\"Experience\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Years workexperience'] = gr.Number(label=\"Years\", value=1)\n",
        "                            inputs_map['Months workexperience'] = gr.Number(label=\"Months\", value=0)\n",
        "\n",
        "                    with gr.Accordion(\"Additional Info\", open=False):\n",
        "                        inputs_map['Number of nominations'] = gr.Number(label=\"Number of Nominations\", value=0)\n",
        "\n",
        "                    all_inputs = [inputs_map[col] for col in ALL_FEATURE_COLUMNS]\n",
        "                    submit_btn = gr.Button(\" Generate Full AI Report\", variant=\"primary\")\n",
        "                    status_output = gr.Textbox(label=\"System Status\", interactive=False, value=\"App starting...\")\n",
        "\n",
        "                # --- RIGHT COLUMN: COMMAND CENTER ---\n",
        "                with gr.Column(scale=7):\n",
        "                    gr.Markdown(\"### 2. Analysis & Recommendations Command Center\")\n",
        "                    with gr.Row():\n",
        "                        prediction_output = gr.Textbox(label=\"Hire Probability\", value=\"N/A\", scale=1)\n",
        "                        category_output = gr.Markdown(value=\"*Categorization*\")\n",
        "                        success_box = gr.Markdown(label=\"Success Stories\", value=\"*Run a report to find matches*\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        skill_chart_output = gr.Plot(label=\"Skills Radar\")\n",
        "                        action_plan_output = gr.Markdown(value=\"*Action Plan will appear here*\", label=\"AI Action Plan\")\n",
        "\n",
        "        # --- PAGE 2: MOCK INTERVIEW ---\n",
        "        with gr.TabItem(\" AI Mock Interview\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    role_input = gr.Textbox(label=\"Target Role\", value=\"Data Analyst\")\n",
        "                    start_interview_btn = gr.Button(\"Start New Interview\", variant=\"primary\")\n",
        "                    interview_clear_btn = gr.Button(\"Clear Chat\")\n",
        "                with gr.Column(scale=3):\n",
        "                    interview_chatbot = gr.Chatbot(height=500)\n",
        "                    interview_msg = gr.Textbox(label=\"Your Answer\")\n",
        "                    interview_send = gr.Button(\"Send Answer\")\n",
        "\n",
        "            start_interview_btn.click(start_mock_interview, inputs=[role_input, interview_chatbot], outputs=[interview_chatbot])\n",
        "            interview_msg.submit(continue_mock_interview, inputs=[interview_msg, interview_chatbot, role_input], outputs=[interview_msg, interview_chatbot])\n",
        "            interview_send.click(continue_mock_interview, inputs=[interview_msg, interview_chatbot, role_input], outputs=[interview_msg, interview_chatbot])\n",
        "            interview_clear_btn.click(lambda: [], outputs=[interview_chatbot])\n",
        "\n",
        "        # --- PAGE 3: DASHBOARD ---\n",
        "        with gr.TabItem(\" Executive Analytics Dashboard\"):\n",
        "            gr.Markdown(\"### Global User Population Analysis\")\n",
        "            global_dashboard_plot = gr.Plot(label=\"Global Dashboard\")\n",
        "            refresh_button = gr.Button(\"Refresh Global Dashboard\", variant=\"secondary\")\n",
        "            refresh_button.click(generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "\n",
        "        # --- PAGE 4: CHAT ---\n",
        "        with gr.TabItem(\" Follow-Up Counselor Chat\"):\n",
        "            gr.Markdown(\"### Ask follow-up questions about the report you just generated in Tab 1.\")\n",
        "            chat_window = gr.Chatbot(label=\"Follow-up Chat\", height=500)\n",
        "            with gr.Row():\n",
        "                chat_textbox = gr.Textbox(show_label=False, placeholder=\"Ask me anything about the report above...\", scale=8)\n",
        "                chat_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "            chat_btn.click(call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "            chat_textbox.submit(call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "\n",
        "        # --- PAGE 5: NEW CANDIDATE ANALYSIS ---\n",
        "        with gr.TabItem(\" New Candidate Analysis\"):\n",
        "            gr.Markdown(\"### Analyze the recently submitted new candidates.\")\n",
        "            analyze_new_candidates_btn = gr.Button(\"Run New Candidate Analysis\", variant=\"primary\")\n",
        "            new_candidate_summary_output = gr.Markdown(\"### Analysis Summary will appear here.\")\n",
        "            new_candidate_plot_output = gr.Plot(label=\"Potential Category Distribution\")\n",
        "            new_candidate_df_output = gr.DataFrame(label=\"Processed New Candidate Data (with Predictions)\")\n",
        "\n",
        "            analyze_new_candidates_btn.click(\n",
        "                fn=get_new_candidate_analysis_data,\n",
        "                outputs=[new_candidate_df_output, new_candidate_plot_output, new_candidate_summary_output]\n",
        "            )\n",
        "\n",
        "\n",
        "    # --- FINAL GLOBAL CONNECTIONS ---\n",
        "    demo.load(fn=on_app_load, outputs=[status_output]).then(fn=generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "    parse_btn.click(parse_resume, inputs=[resume_file] + all_inputs, outputs=all_inputs)\n",
        "    autofill_btn.click(autofill_sample_data, outputs=all_inputs) # Connect autofill button\n",
        "    submit_btn.click(\n",
        "        fn=process_new_user_submission,\n",
        "        inputs=all_inputs,\n",
        "        outputs=[prediction_output, category_output, skill_chart_output, action_plan_output, report_context_state, global_dashboard_plot, success_box]\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Launching Ultimate App...\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "gTzcIiaNyONg",
        "outputId": "6d1a74c4-b232-4d85-c272-e79b30b31999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Ultimate App...\n",
            "Launching Ultimate App...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b7bd58aaee9970ba78.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b7bd58aaee9970ba78.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\n",
            "...XGBoost Models loaded successfully.\n",
            "...Loaded 72641 users for Global Dashboard.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9"
      ],
      "metadata": {
        "id": "6K-luV5yPuQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10"
      ],
      "metadata": {
        "id": "ylmnRy7SPwoH"
      }
    }
  ]
}