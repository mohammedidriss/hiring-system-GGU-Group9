{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedidriss/hiring-system-GGU-Group9/blob/main/ROI_Calculator_Advanced_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸ“Š Comprehensive ROI Calculator: Cloud vs. OpenShift + Labor Costs\n",
        "# @markdown Run this cell to launch the full calculator with Labor estimation.\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. PRICING CONSTANTS (ESTIMATED PUBLIC PRICING 2024/2025) ---\n",
        "PRICING = {\n",
        "    # --- CLOUD NATIVE (Serverless + Dedicated AI Endpoints) ---\n",
        "    \"AWS Native\": {\n",
        "        \"Type\": \"Cloud Native\",\n",
        "        \"Storage_GB\": 0.023, \"Compute_Unit\": 0.0000166667, \"Request_Unit\": 0.20,\n",
        "        \"LB_Hr\": 0.0225, \"LB_LCU\": 0.008,\n",
        "        \"LLM_In_1k\": 0.00300, \"LLM_Out_1k\": 0.01500, # Bedrock API\n",
        "        \"Training_Hr\": 0.736,          # SageMaker Training (g4dn.xlarge)\n",
        "        \"Inference_Hr\": 0.736,         # SageMaker Real-time Endpoint (g4dn.xlarge)\n",
        "        \"Vector_Hr\": 0.24,             # OpenSearch Serverless\n",
        "        \"Color\": \"#FF9900\"\n",
        "    },\n",
        "    \"Azure Native\": {\n",
        "        \"Type\": \"Cloud Native\",\n",
        "        \"Storage_GB\": 0.018, \"Compute_Unit\": 0.000016, \"Request_Unit\": 0.20,\n",
        "        \"LB_Hr\": 0.025, \"LB_LCU\": 0.005,\n",
        "        \"LLM_In_1k\": 0.00500, \"LLM_Out_1k\": 0.01500, # OpenAI API\n",
        "        \"Training_Hr\": 0.52,           # Azure ML Compute (NC4as T4)\n",
        "        \"Inference_Hr\": 0.52,          # Azure ML Online Endpoint\n",
        "        \"Vector_Hr\": 0.11,             # AI Search\n",
        "        \"Color\": \"#0078D4\"\n",
        "    },\n",
        "    \"GCP Native\": {\n",
        "        \"Type\": \"Cloud Native\",\n",
        "        \"Storage_GB\": 0.020, \"Compute_Unit\": 0.0000240, \"Request_Unit\": 0.40,\n",
        "        \"LB_Hr\": 0.025, \"LB_LCU\": 0.008,\n",
        "        \"LLM_In_1k\": 0.00350, \"LLM_Out_1k\": 0.01050, # Gemini API\n",
        "        \"Training_Hr\": 0.65,           # Vertex AI Training (n1-std-4 + T4)\n",
        "        \"Inference_Hr\": 0.65,          # Vertex AI Endpoint\n",
        "        \"Vector_Hr\": 0.10,             # Vertex Vector Search\n",
        "        \"Color\": \"#34A853\"\n",
        "    },\n",
        "    # --- OPENSHIFT / HYBRID (Infrastructure Based) ---\n",
        "    \"ROSA (AWS)\": {\n",
        "        \"Type\": \"OpenShift\",\n",
        "        \"Storage_GB\": 0.10, \"Compute_Node_Hr\": 0.38,\n",
        "        \"GPU_Node_Hr\": 1.20, # g5.xlarge + ROSA Fee\n",
        "        \"Training_Node_Hr\": 1.20,\n",
        "        \"LB_Hr\": 0.0225, \"Color\": \"#CC0000\"\n",
        "    },\n",
        "    \"ARO (Azure)\": {\n",
        "        \"Type\": \"OpenShift\",\n",
        "        \"Storage_GB\": 0.12, \"Compute_Node_Hr\": 0.42,\n",
        "        \"GPU_Node_Hr\": 1.10, # NC series + ARO Fee\n",
        "        \"Training_Node_Hr\": 1.10,\n",
        "        \"LB_Hr\": 0.025, \"Color\": \"#8a0a0a\"\n",
        "    },\n",
        "    \"OpenShift Self-Managed\": {\n",
        "        \"Type\": \"OpenShift\",\n",
        "        \"Storage_GB\": 0.05, \"Compute_Node_Hr\": 0.15,\n",
        "        \"GPU_Node_Hr\": 0.50, # Hardware Amortization\n",
        "        \"Training_Node_Hr\": 0.50,\n",
        "        \"LB_Hr\": 0.01, \"Color\": \"#212121\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 2. CALCULATION ENGINE ---\n",
        "\n",
        "def calculate_costs(candidates_pm, queries_daily, training_hours,\n",
        "                    extra_storage_gb, manual_app_nodes, manual_gpu_nodes,\n",
        "                    autoscale_enabled,\n",
        "                    # Labor Inputs\n",
        "                    pm_count, pm_rate,\n",
        "                    ds_count, ds_rate,\n",
        "                    de_count, de_rate,\n",
        "                    devops_count, devops_rate):\n",
        "\n",
        "    # --- A. INFRASTRUCTURE & AI COST DRIVERS ---\n",
        "    base_storage_gb = (candidates_pm * 5.0) / 1024\n",
        "    total_storage_gb = base_storage_gb + extra_storage_gb\n",
        "\n",
        "    monthly_requests = (candidates_pm * 15) + (queries_daily * 30 * 2)\n",
        "    compute_seconds = monthly_requests * 2.0\n",
        "\n",
        "    tokens_in_k = ((candidates_pm * 2000) + (queries_daily * 30 * 1000)) / 1000\n",
        "    tokens_out_k = ((candidates_pm * 1000) + (queries_daily * 30 * 500)) / 1000\n",
        "\n",
        "    lb_hours = 730\n",
        "    lb_units = monthly_requests / 1000000\n",
        "\n",
        "    # --- B. LABOR COST CALCULATION ---\n",
        "    cost_pm = pm_count * pm_rate\n",
        "    cost_ds = ds_count * ds_rate\n",
        "    cost_de = de_count * de_rate\n",
        "    cost_devops = devops_count * devops_rate\n",
        "\n",
        "    total_labor = cost_pm + cost_ds + cost_de + cost_devops\n",
        "\n",
        "    results = []\n",
        "    breakdown_data = []\n",
        "\n",
        "    for provider, p in PRICING.items():\n",
        "        row_detail = {\"Provider\": provider}\n",
        "\n",
        "        if p[\"Type\"] == \"Cloud Native\":\n",
        "            # --- CLOUD NATIVE CALC ---\n",
        "            c_store = total_storage_gb * p[\"Storage_GB\"]\n",
        "            c_comp = (compute_seconds * p[\"Compute_Unit\"]) + ((monthly_requests/1e6) * p[\"Request_Unit\"])\n",
        "            c_lb = (lb_hours * p[\"LB_Hr\"]) + (lb_units * 10 * p[\"LB_LCU\"])\n",
        "            c_vec = 730 * p[\"Vector_Hr\"]\n",
        "\n",
        "            # AI Costs\n",
        "            c_llm = (tokens_in_k * p[\"LLM_In_1k\"]) + (tokens_out_k * p[\"LLM_Out_1k\"]) # API\n",
        "            c_train = training_hours * p[\"Training_Hr\"] # Training Job\n",
        "            c_inference = manual_gpu_nodes * 730 * p[\"Inference_Hr\"] # SageMaker/Vertex Endpoint\n",
        "\n",
        "            infra_cost = c_store + c_comp + c_lb + c_vec + c_inference\n",
        "            ai_cost = c_llm + c_train\n",
        "\n",
        "            row_detail.update({\n",
        "                \"Infra Scale\": \"Serverless + Endpoints\",\n",
        "                \"Storage\": c_store, \"Compute\": c_comp, \"Network/LB\": c_lb,\n",
        "                \"Vector DB\": c_vec,\n",
        "                \"AI Training\": c_train,\n",
        "                \"LLM Tokens\": c_llm,\n",
        "                \"Dedicated AI Hosting\": c_inference,\n",
        "                \"Labor Cost\": total_labor\n",
        "            })\n",
        "\n",
        "            results.append({\n",
        "                \"Provider\": provider,\n",
        "                \"Total\": infra_cost + ai_cost + total_labor,\n",
        "                \"Infra\": infra_cost,\n",
        "                \"AI_Services\": ai_cost,\n",
        "                \"Labor\": total_labor,\n",
        "                \"Color\": p[\"Color\"]\n",
        "            })\n",
        "\n",
        "        else:\n",
        "            # --- OPENSHIFT CALC ---\n",
        "            req_capacity = 400000\n",
        "            demand_nodes = int(np.ceil(max(1, monthly_requests / req_capacity)))\n",
        "\n",
        "            demand_gpu = 1\n",
        "            if queries_daily > 8000: demand_gpu = 2\n",
        "            if queries_daily > 20000: demand_gpu = 4\n",
        "\n",
        "            if autoscale_enabled:\n",
        "                app_nodes = max(manual_app_nodes, demand_nodes)\n",
        "                gpu_nodes = max(manual_gpu_nodes, demand_gpu)\n",
        "            else:\n",
        "                app_nodes = max(1, manual_app_nodes)\n",
        "                gpu_nodes = manual_gpu_nodes\n",
        "\n",
        "            c_store = total_storage_gb * p[\"Storage_GB\"]\n",
        "            c_nodes = app_nodes * 730 * p[\"Compute_Node_Hr\"]\n",
        "            c_lb = lb_hours * p[\"LB_Hr\"]\n",
        "            c_gpu_host = gpu_nodes * 730 * p[\"GPU_Node_Hr\"]\n",
        "            c_train = training_hours * p[\"Training_Node_Hr\"]\n",
        "\n",
        "            infra_cost = c_store + c_nodes + c_lb + c_gpu_host\n",
        "            ai_cost = c_train\n",
        "\n",
        "            row_detail.update({\n",
        "                \"Infra Scale\": f\"{app_nodes} App / {gpu_nodes} GPU\",\n",
        "                \"Storage\": c_store, \"Compute\": c_nodes, \"Network/LB\": c_lb,\n",
        "                \"Vector DB\": 0.00,\n",
        "                \"AI Training\": c_train,\n",
        "                \"LLM Tokens\": 0.00,\n",
        "                \"Dedicated AI Hosting\": c_gpu_host,\n",
        "                \"Labor Cost\": total_labor\n",
        "            })\n",
        "\n",
        "            results.append({\n",
        "                \"Provider\": provider,\n",
        "                \"Total\": infra_cost + ai_cost + total_labor,\n",
        "                \"Infra\": infra_cost,\n",
        "                \"AI_Services\": ai_cost,\n",
        "                \"Labor\": total_labor,\n",
        "                \"Color\": p[\"Color\"]\n",
        "            })\n",
        "\n",
        "        row_detail[\"TOTAL MONTHLY\"] = sum([v for k,v in row_detail.items() if isinstance(v, (int, float))])\n",
        "        breakdown_data.append(row_detail)\n",
        "\n",
        "    return pd.DataFrame(results), pd.DataFrame(breakdown_data)\n",
        "\n",
        "# --- 3. GRADIO LOGIC ---\n",
        "\n",
        "def update_dashboard(cand, chat, train, store, app_nodes, gpu_nodes, auto,\n",
        "                     pm_c, pm_r, ds_c, ds_r, de_c, de_r, do_c, do_r):\n",
        "\n",
        "    df_summary, df_detail = calculate_costs(cand, chat, train, store, app_nodes, gpu_nodes, auto,\n",
        "                                            pm_c, pm_r, ds_c, ds_r, de_c, de_r, do_c, do_r)\n",
        "\n",
        "    # 1. Create Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plotting\n",
        "    plot_data = df_summary.set_index('Provider')[['Infra', 'AI_Services', 'Labor']]\n",
        "    # Colors: Infra(GreyBlue), AI(Green), Labor(OrangeRed)\n",
        "    plot_data.plot(kind='bar', stacked=True, ax=ax, color=['#78909C', '#66BB6A', '#FF7043'])\n",
        "\n",
        "    # Styling\n",
        "    ax.set_title('Monthly Total Cost of Ownership (TCO) + Labor', fontsize=14, pad=15)\n",
        "    ax.set_ylabel('Monthly Cost ($)')\n",
        "    ax.set_xlabel('')\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "    ax.legend([\"Infrastructure\", \"AI Services\", \"Labor Team\"], loc='upper left')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Annotations\n",
        "    max_val = df_summary['Total'].max()\n",
        "    ax.set_ylim(0, max_val * 1.25)\n",
        "\n",
        "    for n, total in enumerate(df_summary['Total']):\n",
        "        ax.text(n, total + (max_val*0.02), f\"${total:,.0f}\", ha='center', weight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 2. Format Table (Rounding)\n",
        "    numeric_cols = [\"Storage\", \"Compute\", \"Network/LB\", \"Vector DB\", \"AI Training\", \"LLM Tokens\", \"Dedicated AI Hosting\", \"Labor Cost\", \"TOTAL MONTHLY\"]\n",
        "    df_detail[numeric_cols] = df_detail[numeric_cols].applymap(lambda x: f\"${x:,.2f}\")\n",
        "\n",
        "    # Reorder\n",
        "    final_cols = [\"Provider\", \"Infra Scale\"] + numeric_cols\n",
        "\n",
        "    return fig, df_detail[final_cols]\n",
        "\n",
        "# --- 4. GRADIO INTERFACE ---\n",
        "\n",
        "with gr.Blocks(title=\"Cloud vs OpenShift ROI Calculator + Labor\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ðŸ“Š Real-World ROI Calculator: Tech Stack + Labor Costs\n",
        "    Compare the FULL monthly cost of running your AI Platform, including Infrastructure, AI Services, and **Engineering Labor**.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # LEFT COLUMN: INFRA INPUTS\n",
        "        with gr.Column(scale=1, min_width=300):\n",
        "            gr.Markdown(\"### 1. Workload Drivers\")\n",
        "            w_cand = gr.Slider(500, 50000, value=5000, step=500, label=\"Monthly Candidates Processed\")\n",
        "            w_chat = gr.Slider(100, 25000, value=2000, step=100, label=\"Daily RAG Chat Queries\")\n",
        "            w_train = gr.Slider(0, 200, value=20, step=10, label=\"Monthly Training Hours (GPU)\")\n",
        "\n",
        "            gr.Markdown(\"### 2. Infra Scaling\")\n",
        "            w_store = gr.Slider(0, 10000, value=500, step=100, label=\"Addt'l Storage (GB)\")\n",
        "            w_n_app = gr.Slider(1, 20, value=3, step=1, label=\"App/Compute Nodes\")\n",
        "            w_n_gpu = gr.Slider(0, 10, value=1, step=1, label=\"GPU Nodes / AI Endpoints\")\n",
        "            w_auto = gr.Checkbox(value=True, label=\"âœ… Enable Auto-scaling\")\n",
        "\n",
        "        # MIDDLE COLUMN: LABOR INPUTS\n",
        "        with gr.Column(scale=1, min_width=300):\n",
        "            gr.Markdown(\"### 3. Human Resources (Labor)\")\n",
        "            with gr.Accordion(\"Project Management\", open=True):\n",
        "                l_pm_c = gr.Number(value=1, label=\"Project Managers (Count)\")\n",
        "                l_pm_r = gr.Number(value=10000, label=\"Avg Monthly Cost ($)\")\n",
        "\n",
        "            with gr.Accordion(\"Data Science Team\", open=False):\n",
        "                l_ds_c = gr.Number(value=2, label=\"Data Scientists (Count)\")\n",
        "                l_ds_r = gr.Number(value=12000, label=\"Avg Monthly Cost ($)\")\n",
        "\n",
        "            with gr.Accordion(\"Data Engineering Team\", open=False):\n",
        "                l_de_c = gr.Number(value=1, label=\"Data Engineers (Count)\")\n",
        "                l_de_r = gr.Number(value=11000, label=\"Avg Monthly Cost ($)\")\n",
        "\n",
        "            with gr.Accordion(\"DevOps / Cloud Eng\", open=False):\n",
        "                l_do_c = gr.Number(value=1, label=\"DevOps Engineers (Count)\")\n",
        "                l_do_r = gr.Number(value=11000, label=\"Avg Monthly Cost ($)\")\n",
        "\n",
        "            btn = gr.Button(\"ðŸš€ Calculate Total ROI\", variant=\"primary\")\n",
        "\n",
        "    # BOTTOM ROW: RESULTS\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### ðŸ’° Total Cost of Ownership (TCO)\")\n",
        "            out_plot = gr.Plot(label=\"Cost Comparison\")\n",
        "\n",
        "            gr.Markdown(\"### ðŸ§¾ Detailed Cost Breakdown\")\n",
        "            out_table = gr.Dataframe(\n",
        "                headers=[\"Provider\", \"Scale\", \"Storage\", \"Compute\", \"Net\", \"Vector\", \"Train\", \"LLM\", \"AI Host\", \"Labor\", \"Total\"],\n",
        "                label=\"Detailed Receipt\"\n",
        "            )\n",
        "\n",
        "    # Event Listener\n",
        "    inputs = [w_cand, w_chat, w_train, w_store, w_n_app, w_n_gpu, w_auto,\n",
        "              l_pm_c, l_pm_r, l_ds_c, l_ds_r, l_de_c, l_de_r, l_do_c, l_do_r]\n",
        "    outputs = [out_plot, out_table]\n",
        "\n",
        "    # Update on load and click\n",
        "    btn.click(update_dashboard, inputs, outputs)\n",
        "    demo.load(update_dashboard, inputs, outputs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1458990673.py:231: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"Cloud vs OpenShift ROI Calculator + Labor\", theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://913d0e7ba8aa01dde5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://913d0e7ba8aa01dde5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "qC8yW-UCYBPo",
        "outputId": "f6dc2c1e-e81b-4bcc-ad31-5b67aafcce94"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}