{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLNjfkHfkkoZzMuBLlccit",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedidriss/hiring-system-GGU-Group9/blob/main/Course4_Hiring_system_v1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1"
      ],
      "metadata": {
        "id": "w5cv58GEPa7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkXBCetjbMSw",
        "outputId": "0fed0a5e-bc1f-45cd-a94f-b0faff71291e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries: gradio, xgboost, shap, openpyxl, httpx...\n",
            "Installations complete.\n"
          ]
        }
      ],
      "source": [
        "# This cell installs all the external libraries we need.\n",
        "# I have ADDED 'httpx' which is the Python library for making API calls.\n",
        "print(\"Installing required libraries: gradio, xgboost, shap, openpyxl, httpx...\")\n",
        "!pip install -q gradio xgboost shap openpyxl httpx\n",
        "print(\"Installations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2"
      ],
      "metadata": {
        "id": "Wf2JtT7zPfLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell imports all the tools.\n",
        "# I have ADDED 'httpx' (for API calls) and 'asyncio'.\n",
        "\n",
        "# For data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json # Make sure json is imported\n",
        "\n",
        "# For the Web App (UI)\n",
        "import gradio as gr\n",
        "\n",
        "# For saving/loading models and files\n",
        "import joblib\n",
        "import datetime\n",
        "\n",
        "# For AI Model Building (Preprocessing)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# For AI Model Training (XGBoost)\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# For AI Model Explainability (SHAP)\n",
        "import shap\n",
        "\n",
        "# For making API calls (replaces fetch)\n",
        "import httpx\n",
        "import asyncio\n",
        "\n",
        "# To hide common warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported. Ready to mount Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf3tmiZOmjgv",
        "outputId": "426038cf-815f-4719-b29d-f345f3af4491"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported. Ready to mount Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3"
      ],
      "metadata": {
        "id": "60jpIQ6FPg2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell connects your Colab notebook to your Google Drive\n",
        "# to access your data files.\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "# This will pop up an authorization window.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully at /content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyFxY6-Lmlsw",
        "outputId": "fdd6d876-bc25-4b06-e230-086ca77f5503"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4"
      ],
      "metadata": {
        "id": "FCUyHL3WPibz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell defines all our file paths and creates the \"Master Dataset.\"\n",
        "# It reads your two Excel sheets (from the same file), merges them,\n",
        "# creates the 'TARGET_HIRED' label, and saves this new\n",
        "# \"master\" file back to your Drive.\n",
        "\n",
        "print(\"--- Phase 1: Creating Master Training Dataset ---\")\n",
        "\n",
        "# --- 1. Define File Paths ---\n",
        "# This is your original Excel file\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
        "FEATURES_FILE_PATH = f\"{DRIVE_PATH}SampleData.xlsx\"\n",
        "FEATURES_SHEET_NAME = \"sample_jsdrilldown\"\n",
        "OUTCOMES_SHEET_NAME = \"Sample_Workshop_app\" # The other sheet in your file\n",
        "MASTER_DATASET_PATH = f\"{DRIVE_PATH}master_training_dataset.csv\"\n",
        "\n",
        "# --- 2. Define the Function ---\n",
        "def create_master_dataset():\n",
        "    \"\"\"\n",
        "    Merges features and outcomes, calculates target labels,\n",
        "    and saves a new \"master\" dataset.\n",
        "    \"\"\"\n",
        "    print(f\"Loading features from: {FEATURES_SHEET_NAME}\")\n",
        "    try:\n",
        "        df_features = pd.read_excel(FEATURES_FILE_PATH, sheet_name=FEATURES_SHEET_NAME)\n",
        "        print(f\"Loaded {len(df_features)} feature rows.\")\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR loading features sheet: {e} !!!---\")\n",
        "        print(\"Please check: 1. File path is correct. 2. Sheet name is correct. 3. 'openpyxl' is installed.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading outcomes from: {OUTCOMES_SHEET_NAME}\")\n",
        "    try:\n",
        "        # We only need the 'Journey id' and the 'Status' columns\n",
        "        df_outcomes = pd.read_excel(FEATURES_FILE_PATH, sheet_name=OUTCOMES_SHEET_NAME, usecols=['Journey id', 'Status'])\n",
        "        print(f\"Loaded {len(df_outcomes)} outcome rows.\")\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR loading outcomes sheet: {e} !!!---\")\n",
        "        return None\n",
        "\n",
        "    # --- 3. Create the Classification Label (The \"Prediction\") ---\n",
        "    # Use 'Status' == 'In work' as our \"Hired\" (1) or \"Not Hired\" (0) label\n",
        "    df_outcomes_clean = df_outcomes.drop_duplicates(subset=['Journey id'])\n",
        "    df_outcomes_clean['TARGET_HIRED'] = np.where(df_outcomes_clean['Status'] == 'In work', 1, 0)\n",
        "\n",
        "    # --- 4. Merge Features and Labels ---\n",
        "    # We use a 'left' merge to keep all candidates from the features sheet\n",
        "    # and add the 'TARGET_HIRED' label to them.\n",
        "    #\n",
        "    print(\"Merging features and labels...\")\n",
        "    df_master = pd.merge(\n",
        "        df_features,\n",
        "        df_outcomes_clean[['Journey id', 'TARGET_HIRED']],\n",
        "        on='Journey id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Fill any candidate who has no outcome data with 0 (Not Hired)\n",
        "    df_master['TARGET_HIRED'] = df_master['TARGET_HIRED'].fillna(0).astype(int)\n",
        "\n",
        "    print(f\"Master dataset created with {len(df_master)} rows.\")\n",
        "\n",
        "    # --- 5. Save the new master file ---\n",
        "    df_master.to_csv(MASTER_DATASET_PATH, index=False)\n",
        "    print(f\"Master dataset saved to: {MASTER_DATASET_PATH}\")\n",
        "    return df_master\n",
        "\n",
        "# --- 6. RUN THE FUNCTION ---\n",
        "master_df = create_master_dataset()\n",
        "if master_df is not None:\n",
        "    print(f\"We have {master_df['TARGET_HIRED'].sum()} 'Hired' users to train on.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs_qIuufmxhi",
        "outputId": "80d2df22-6f7e-40dc-ea46-ccb3e53ab0bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Creating Master Training Dataset ---\n",
            "Loading features from: sample_jsdrilldown\n",
            "Loaded 2000 feature rows.\n",
            "Loading outcomes from: Sample_Workshop_app\n",
            "Loaded 5375 outcome rows.\n",
            "Merging features and labels...\n",
            "Master dataset created with 2000 rows.\n",
            "Master dataset saved to: /content/drive/MyDrive/master_training_dataset.csv\n",
            "We have 1000 'Hired' users to train on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5"
      ],
      "metadata": {
        "id": "Oc9We-v6Pj8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell re-builds the \"AI Brain\" (XGBoost Model).\n",
        "# We are REMOVING the \"leaky\" columns 'Work status' and 'Stream'\n",
        "# from the AI_FEATURE_COLUMNS list.\n",
        "\n",
        "print(\"--- Phase 2: Building the AI Brain (XGBoost Model) ---\")\n",
        "\n",
        "# Define the brain file path here, similar to MASTER_DATASET_PATH\n",
        "BRAIN_FILE_PATH = f\"{DRIVE_PATH}ai_brain_pipeline.joblib\"\n",
        "\n",
        "if 'master_df' not in locals() or master_df.empty:\n",
        "    print(\"ERROR: 'master_df' not found. Please run Cell 4 first.\")\n",
        "else:\n",
        "    TARGET_COLUMN = 'TARGET_HIRED'\n",
        "\n",
        "    # --- THIS IS THE FIX ---\n",
        "    # We have removed 'Work status' and 'Stream' from this list.\n",
        "    # The AI will now be forced to learn from *real* features,\n",
        "    # not the \"cheat code\" answer.\n",
        "    AI_FEATURE_COLUMNS = [\n",
        "        'gender', 'Age', 'Marital status', 'JS Town', 'JS Town distrinct',\n",
        "        # 'Stream', (REMOVED - This is a program, not a candidate attribute)\n",
        "        # 'Work status', (REMOVED - This is the answer!)\n",
        "        'Salary expectations', 'Highest Qualification', 'Highest major',\n",
        "        'Shifts', 'Working environment', 'Skills English', 'Skills MS Word',\n",
        "        'Skills MS Excel', 'Skills MS Powerpoint', 'Transport available', 'Driving license',\n",
        "        'Job goal 1', 'Job goal 2', 'Job goal 3', 'Years workexperience',\n",
        "        'Months workexperience', 'Communication skills', 'Has CV (SWS)'\n",
        "    ]\n",
        "    # ---------------------\n",
        "\n",
        "    # Check if all columns are in master_df\n",
        "    missing_cols = [col for col in AI_FEATURE_COLUMNS if col not in master_df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"---!!! ERROR: The following columns are missing from 'master_df': {missing_cols} !!!---\")\n",
        "        print(\"Please check the 'sample_jsdrilldown' sheet and the 'AI_FEATURE_COLUMNS' list.\")\n",
        "        raise ValueError(\"Missing feature columns in master_df\")\n",
        "\n",
        "    # Automatically separate features by type\n",
        "    numeric_features = master_df[AI_FEATURE_COLUMNS].select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = master_df[AI_FEATURE_COLUMNS].select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "    # Manually define ordinal (ordered) features\n",
        "    ordinal_features = [\n",
        "        'Skills English', 'Skills MS Word', 'Skills MS Excel', 'Skills MS Powerpoint', 'Communication skills'\n",
        "    ]\n",
        "    skill_levels = ['N_A', 'Beginner', 'Good', 'Excellent']\n",
        "\n",
        "    # Remove ordinal features from the main categorical list\n",
        "    categorical_features = [col for col in categorical_features if col not in ordinal_features]\n",
        "\n",
        "    print(f\"Identified {len(numeric_features)} numeric, {len(ordinal_features)} ordinal, {len(categorical_features)} categorical features.\")\n",
        "    print(f\"Total features being used for training: {len(AI_FEATURE_COLUMNS)}\")\n",
        "\n",
        "    # --- Build Preprocessing Pipelines ---\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    ordinal_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='N_A')),\n",
        "        ('encoder', OrdinalEncoder(categories=[skill_levels] * len(ordinal_features), handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # --- Combine Pipelines ---\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('ord', ordinal_transformer, ordinal_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # --- Create the Full AI Brain Pipeline ---\n",
        "    print(\"\\nDataset is balanced (1000 Hired / 1000 Not Hired).\")\n",
        "    print(\"Using default 'scale_pos_weight=1'.\")\n",
        "\n",
        "    ai_brain_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', xgb.XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False,\n",
        "            scale_pos_weight=1,  # Set to 1 because our data is balanced!\n",
        "            random_state=42\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # --- Train the AI Brain ---\n",
        "    print(\"\\nSplitting data and training model...\")\n",
        "    X = master_df[AI_FEATURE_COLUMNS]\n",
        "    y = master_df[TARGET_COLUMN]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    ai_brain_pipeline.fit(X_train, y_train)\n",
        "    print(\"...Training complete.\")\n",
        "\n",
        "    # --- Evaluate the AI Brain ---\n",
        "    print(\"\\n--- Model Performance Evaluation (No Leakage) ---\")\n",
        "    preds = ai_brain_pipeline.predict(X_test)\n",
        "    print(f\"Accuracy on Test Data: {accuracy_score(y_test, preds):.2%}\")\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "    # --- Save the AI Brain to Google Drive ---\n",
        "    joblib.dump(ai_brain_pipeline, BRAIN_FILE_PATH)\n",
        "    print(\"\\n---------------------------------\")\n",
        "    print(f\"SUCCESS: AI Brain (new NON-LEAKING version) saved to: {BRAIN_FILE_PATH}\")\n",
        "    print(\"---------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFNzkhM-msur",
        "outputId": "d5674750-aa48-4e3b-83cf-53f1283fb1f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 2: Building the AI Brain (XGBoost Model) ---\n",
            "Identified 3 numeric, 5 ordinal, 15 categorical features.\n",
            "Total features being used for training: 23\n",
            "\n",
            "Dataset is balanced (1000 Hired / 1000 Not Hired).\n",
            "Using default 'scale_pos_weight=1'.\n",
            "\n",
            "Splitting data and training model...\n",
            "...Training complete.\n",
            "\n",
            "--- Model Performance Evaluation (No Leakage) ---\n",
            "Accuracy on Test Data: 66.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.67      0.66       200\n",
            "           1       0.66      0.65      0.66       200\n",
            "\n",
            "    accuracy                           0.66       400\n",
            "   macro avg       0.66      0.66      0.66       400\n",
            "weighted avg       0.66      0.66      0.66       400\n",
            "\n",
            "\n",
            "---------------------------------\n",
            "SUCCESS: AI Brain (new NON-LEAKING version) saved to: /content/drive/MyDrive/ai_brain_pipeline.joblib\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6"
      ],
      "metadata": {
        "id": "qcofwQ2GPlfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell builds the \"Action Plan Generator\" (SHAP Explainer).\n",
        "# It loads the model we just saved, trains a SHAP explainer\n",
        "# on it, and saves the explainer to your Drive.\n",
        "\n",
        "print(\"--- Phase 3: Building the Action Plan Generator (SHAP) ---\")\n",
        "\n",
        "# --- 1. Load the Saved AI Brain ---\n",
        "# (BRAIN_FILE_PATH and EXPLAINER_FILE_PATH were defined in Cell 3)\n",
        "# FIX: Define EXPLAINER_FILE_PATH here\n",
        "EXPLAINER_FILE_PATH = f\"{DRIVE_PATH}shap_explainer.joblib\"\n",
        "\n",
        "try:\n",
        "    ai_brain_pipeline = joblib.load(BRAIN_FILE_PATH)\n",
        "    print(f\"Successfully loaded AI Brain from: {BRAIN_FILE_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR loading 'ai_brain_pipeline.joblib': {e} !!!---\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Separate Pipeline Components ---\n",
        "preprocessor = ai_brain_pipeline.named_steps['preprocessor']\n",
        "model = ai_brain_pipeline.named_steps['classifier']\n",
        "\n",
        "# --- 3. Get Feature Names (FIXED METHOD) ---\n",
        "# We ask the preprocessor for its *actual* output names\n",
        "print(\"Getting feature names directly from the preprocessor...\")\n",
        "try:\n",
        "    all_transformed_feature_names = preprocessor.get_feature_names_out().tolist()\n",
        "    print(f\"Successfully got {len(all_transformed_feature_names)} feature names.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting feature names: {e}.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. Transform Training Data (FIXED METHOD) ---\n",
        "print(\"Transforming training data for SHAP explainer...\")\n",
        "# We must re-create X_train and X_test from Cell 5\n",
        "# so the explainer can use them as a background dataset\n",
        "# (AI_FEATURE_COLUMNS and master_df are still in memory from Cell 5)\n",
        "X = master_df[AI_FEATURE_COLUMNS]\n",
        "y = master_df[TARGET_COLUMN]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# We DON'T call .toarray() because the pipeline already outputs a dense array\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "print(\"...Data transformed successfully (dense array).\")\n",
        "\n",
        "# --- 5. Build the SHAP Explainer ---\n",
        "print(\"Building SHAP TreeExplainer...\")\n",
        "#\n",
        "explainer = shap.TreeExplainer(model, X_train_transformed)\n",
        "print(\"...Explainer built successfully.\")\n",
        "\n",
        "# --- 6. Save the SHAP Explainer ---\n",
        "joblib.dump(explainer, EXPLAINER_FILE_PATH)\n",
        "print(f\"SUCCESS: SHAP Explainer saved to: {EXPLAINER_FILE_PATH}\")\n",
        "\n",
        "# --- 7. Define Test Function ---\n",
        "def generate_action_plan_test(new_user_data_df):\n",
        "    prediction_proba = ai_brain_pipeline.predict_proba(new_user_data_df)[0]\n",
        "    hire_probability = prediction_proba[1]\n",
        "    prediction_raw = ai_brain_pipeline.predict(new_user_data_df)[0]\n",
        "\n",
        "    # We DON'T call .toarray() here\n",
        "    user_transformed = preprocessor.transform(new_user_data_df)\n",
        "    shap_values = explainer.shap_values(user_transformed)\n",
        "\n",
        "    # This will now work\n",
        "    df_shap = pd.DataFrame(shap_values, columns=all_transformed_feature_names).iloc[0].T\n",
        "    df_shap = df_shap.reset_index(); df_shap.columns = ['Feature', 'SHAP_Value']\n",
        "    df_shap['abs_impact'] = df_shap['SHAP_Value'].abs()\n",
        "    df_shap = df_shap.sort_values(by='abs_impact', ascending=False)\n",
        "\n",
        "    top_factors = df_shap.head(5)\n",
        "\n",
        "    action_plan = \"--- (TEST) Recommended Action Plan ---\\n\"\n",
        "    for _, row in top_factors.iterrows():\n",
        "        sign = \"[+]\" if row['SHAP_Value'] > 0 else \"[-]\"\n",
        "        action_plan += f\"  {sign} {row['Feature']} (Impact: {row['SHAP_Value']:.2f})\\n\"\n",
        "    action_plan += f\"\\n  Prediction: {'Hired' if prediction_raw == 1 else 'Not Hired'} ({hire_probability:.1%})\"\n",
        "    return action_plan\n",
        "\n",
        "# --- 8. Test the Explainer ---\n",
        "print(\"\\n--- TESTING THE ACTION PLAN GENERATOR ---\")\n",
        "# We use X_test, which we just created in this cell\n",
        "sample_user_df = X_test.iloc[0:1]\n",
        "true_label = y_test.iloc[0]\n",
        "print(f\"Generating plan for a sample user. (True Label: {'Hired' if true_label == 1 else 'Not Hired'})...\")\n",
        "print(generate_action_plan_test(sample_user_df))\n",
        "print(\"---------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J34l0WQlu-22",
        "outputId": "7d2dc24a-bfa6-4370-d9f6-68b8605440d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 3: Building the Action Plan Generator (SHAP) ---\n",
            "Successfully loaded AI Brain from: /content/drive/MyDrive/ai_brain_pipeline.joblib\n",
            "Getting feature names directly from the preprocessor...\n",
            "Successfully got 297 feature names.\n",
            "Transforming training data for SHAP explainer...\n",
            "...Data transformed successfully (dense array).\n",
            "Building SHAP TreeExplainer...\n",
            "...Explainer built successfully.\n",
            "SUCCESS: SHAP Explainer saved to: /content/drive/MyDrive/shap_explainer.joblib\n",
            "\n",
            "--- TESTING THE ACTION PLAN GENERATOR ---\n",
            "Generating plan for a sample user. (True Label: Not Hired)...\n",
            "--- (TEST) Recommended Action Plan ---\n",
            "  [+] num__Age (Impact: 0.39)\n",
            "  [-] cat__JS Town_Dammam (Impact: -0.31)\n",
            "  [-] num__Months workexperience (Impact: -0.29)\n",
            "  [+] cat__JS Town distrinct_Central (Impact: 0.24)\n",
            "  [-] ord__Skills English (Impact: -0.23)\n",
            "\n",
            "  Prediction: Not Hired (43.0%)\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7"
      ],
      "metadata": {
        "id": "A1f1Wm94Pnkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the FINAL, COMBINED cell for the application.\n",
        "# It REPLACES all previous versions.\n",
        "# Fixed: Removed invalid 'scale' argument from gr.Markdown.\n",
        "# Fixed: Cleaned up layout logic.\n",
        "\n",
        "import datetime\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import json\n",
        "import httpx\n",
        "import asyncio\n",
        "from google.colab import userdata\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "print(\"--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\")\n",
        "\n",
        "# --- 1. Load API Key from Colab Secrets ---\n",
        "try:\n",
        "    MY_GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    if not MY_GOOGLE_API_KEY:\n",
        "        raise ValueError(\"API Key is empty or not found.\")\n",
        "    print(\"Successfully loaded Google API Key from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"---!!! CRITICAL ERROR: API KEY NOT FOUND: {e} !!!---\")\n",
        "    print(\"Please go to the 'Key' (ðŸ”‘) icon on the left and add your API key as a secret named 'GOOGLE_API_KEY'\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Define Global Variables & AI Models ---\n",
        "ai_models = {\"pipeline\": None, \"explainer\": None, \"feature_names\": None}\n",
        "global_user_df = pd.DataFrame()\n",
        "\n",
        "ALL_FEATURE_COLUMNS = [\n",
        "    'gender', 'Age', 'Marital status', 'JS Town', 'JS Town distrinct',\n",
        "    'Salary expectations', 'Highest Qualification', 'Highest major',\n",
        "    'Shifts', 'Working environment', 'Skills English', 'Skills MS Word',\n",
        "    'Skills MS Excel', 'Skills MS Powerpoint', 'Transport available', 'Driving license',\n",
        "    'Job goal 1', 'Job goal 2', 'Job goal 3', 'Years workexperience',\n",
        "    'Months workexperience', 'Communication skills', 'Has CV (SWS)'\n",
        "]\n",
        "\n",
        "try:\n",
        "    if 'master_df' in locals() and not master_df.empty:\n",
        "        initial_profile_dict = {col: \"Unknown\" if not np.issubdtype(master_df[col].dtype, np.number) else 0 for col in ALL_FEATURE_COLUMNS}\n",
        "    else:\n",
        "        initial_profile_dict = {col: 0 if col in ['Age', 'Years workexperience', 'Months workexperience'] else 'Unknown' for col in ALL_FEATURE_COLUMNS}\n",
        "except NameError:\n",
        "    initial_profile_dict = {col: 0 if col in ['Age', 'Years workexperience', 'Months workexperience'] else 'Unknown' for col in ALL_FEATURE_COLUMNS}\n",
        "\n",
        "\n",
        "# --- 3. Define Dropdown Choices ---\n",
        "GENDER_CHOICES = ['Male', 'Female', 'Other', 'Prefer not to say']\n",
        "MARITAL_CHOICES = ['Single', 'Married', 'Divorced', 'Widowed', 'Other']\n",
        "SALARY_CHOICES = ['< 3,000SAR', '3,000 - 5,000SAR', '5,000 - 7,000SAR', '> 7,000SAR']\n",
        "QUALIFICATION_CHOICES = ['High School', 'Diploma', 'Bachelor', 'Masters', 'Doctorate']\n",
        "SHIFTS_CHOICES = ['No preference', 'Straight shifts', 'Rotating shifts']\n",
        "ENVIRONMENT_CHOICES = ['Mixed', 'Flexible', 'On-site', 'Remote']\n",
        "SKILL_LEVEL_CHOICES = ['N_A', 'Beginner', 'Good', 'Excellent']\n",
        "YES_NO_CHOICES = ['Yes', 'No']\n",
        "COMMUNICATION_CHOICES = ['Beginner', 'Good', 'Excellent']\n",
        "\n",
        "\n",
        "# --- 4. Load XGBoost AI Brain ---\n",
        "def on_app_load():\n",
        "    global ai_models, global_user_df\n",
        "    print(\"Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\")\n",
        "    try:\n",
        "        ai_models[\"pipeline\"] = joblib.load(BRAIN_FILE_PATH)\n",
        "        ai_models[\"explainer\"] = joblib.load(EXPLAINER_FILE_PATH)\n",
        "        preprocessor = ai_models[\"pipeline\"].named_steps['preprocessor']\n",
        "        ai_models[\"feature_names\"] = preprocessor.get_feature_names_out().tolist()\n",
        "        print(\"...XGBoost Models loaded successfully.\")\n",
        "        global_user_df = pd.read_csv(MASTER_DATASET_PATH)\n",
        "        print(f\"...Loaded {len(global_user_df)} users for Global Dashboard.\")\n",
        "        return \"System Ready. AI Models and Global Data loaded.\"\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! CRITICAL ERROR LOADING MODELS: {e} !!!---\")\n",
        "        return f\"CRITICAL ERROR: Could not load AI models or data. {e}\"\n",
        "\n",
        "# --- 5. Helper Functions ---\n",
        "def format_feature_name_for_llm(feature_name):\n",
        "    name = feature_name.replace(\"cat__\", \"\").replace(\"ord__\", \"\").replace(\"num__\", \"\")\n",
        "    parts = name.split('_', 1)\n",
        "    if len(parts) == 2:\n",
        "        if \"Skills\" in parts[0]: return parts[0].replace(\"Skills \", \"\")\n",
        "        if \"Job goal\" in parts[0]: return \"Job Goal Setting\"\n",
        "        return parts[0]\n",
        "    return name\n",
        "\n",
        "def run_xgb_prediction_and_get_report(profile_dict):\n",
        "    print(\"XGBoost Brain: Running prediction...\")\n",
        "    try:\n",
        "        if ai_models[\"pipeline\"] is None: return \"Error: XGBoost model not loaded.\", \"Unknown\", 0.0\n",
        "        new_user_df = pd.DataFrame([profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "        pipeline = ai_models[\"pipeline\"]\n",
        "        explainer = ai_models[\"explainer\"]\n",
        "        feature_names = ai_models[\"feature_names\"]\n",
        "        prediction_proba = pipeline.predict_proba(new_user_df)[0]\n",
        "        hire_probability = prediction_proba[1]\n",
        "        user_transformed = pipeline.named_steps['preprocessor'].transform(new_user_df)\n",
        "        shap_values = explainer.shap_values(user_transformed)\n",
        "        df_shap = pd.DataFrame(shap_values, columns=feature_names).iloc[0].T\n",
        "        df_shap = df_shap.reset_index(); df_shap.columns = ['Feature', 'SHAP_Value']\n",
        "        df_shap['abs_impact'] = df_shap['SHAP_Value'].abs()\n",
        "        df_negatives = df_shap[df_shap['SHAP_Value'] < 0].sort_values(by='SHAP_Value', ascending=True)\n",
        "        top_weakness_topic = \"General Profile Improvement\"\n",
        "        if not df_negatives.empty:\n",
        "            top_weakness_internal_name = df_negatives.iloc[0]['Feature']\n",
        "            top_weakness_topic = format_feature_name_for_llm(top_weakness_internal_name)\n",
        "        report = f\"STATISTICAL ANALYSIS REPORT:\\n\"\n",
        "        report += f\"Predicted Hire Probability: {hire_probability:.1%}\\n\"\n",
        "        report += \"Top 5 Most Important Factors:\\n\"\n",
        "        for _, row in df_shap.sort_values(by='abs_impact', ascending=False).head(5).iterrows():\n",
        "            sign = \"POSITIVE\" if row['SHAP_Value'] > 0 else \"NEGATIVE\"\n",
        "            report += f\"  - Factor: {row['Feature']}, Impact: {sign}\\n\"\n",
        "        print(f\"...XGBoost Brain: Report generated. Top weakness: {top_weakness_topic}\")\n",
        "        return report, top_weakness_topic, hire_probability\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR during XGBoost prediction: {e} !!!---\")\n",
        "        return f\"Error during XGBoost prediction: {e}\", \"Unknown\", 0.0\n",
        "\n",
        "# --- 6. Generate Individual Charts ---\n",
        "def generate_individual_charts(profile_dict):\n",
        "    print(\"Generating individual skills radar chart...\")\n",
        "    skills = {\n",
        "        'English': profile_dict.get('Skills English', 'N_A'),\n",
        "        'MS Word': profile_dict.get('Skills MS Word', 'N_A'),\n",
        "        'MS Excel': profile_dict.get('Skills MS Excel', 'N_A'),\n",
        "        'MS Powerpoint': profile_dict.get('Skills MS Powerpoint', 'N_A'),\n",
        "        'Communication': profile_dict.get('Communication skills', 'N_A')\n",
        "    }\n",
        "    level_map = {'N_A': 0, 'Beginner': 1, 'Good': 2, 'Excellent': 3}\n",
        "    values = [level_map.get(v, 0) for v in skills.values()]\n",
        "    labels = list(skills.keys())\n",
        "    N = len(labels)\n",
        "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
        "    values += values[:1]\n",
        "    angles += angles[:1]\n",
        "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
        "    ax.fill(angles, values, color='blue', alpha=0.25)\n",
        "    ax.plot(angles, values, color='blue', linewidth=2)\n",
        "    ax.set_yticklabels(['', 'Beginner', 'Good', 'Excellent'])\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_title(\"Candidate Skills Profile\", size=15)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# --- 7. Generate Global Dashboard ---\n",
        "def generate_global_dashboard():\n",
        "    global global_user_df\n",
        "    print(\"Generating global dashboard...\")\n",
        "    if global_user_df.empty:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.text(0.5, 0.5, \"Global user data not loaded.\", ha='center')\n",
        "        return fig\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    gs = fig.add_gridspec(2, 3) # 2 rows, 3 columns\n",
        "\n",
        "    try:\n",
        "        # Chart 1: Domain\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        domain_counts = global_user_df['Stream'].value_counts()\n",
        "        ax1.bar(domain_counts.index, domain_counts.values, color='teal')\n",
        "        ax1.set_title('Candidates by Domain (Stream)')\n",
        "        ax1.tick_params(axis='x', rotation=25)\n",
        "\n",
        "        # Chart 2: Scatter Plot\n",
        "        ax2 = fig.add_subplot(gs[0, 1:])\n",
        "        years = pd.to_numeric(global_user_df['Years workexperience'].fillna(0), errors='coerce').fillna(0)\n",
        "        months = pd.to_numeric(global_user_df['Months workexperience'].fillna(0), errors='coerce').fillna(0)\n",
        "        global_user_df['TotalYears'] = years + (months / 12)\n",
        "\n",
        "        streams = global_user_df['Stream'].dropna().unique()\n",
        "        stream_map = {stream: i for i, stream in enumerate(streams)}\n",
        "        global_user_df['Stream_Num'] = global_user_df['Stream'].map(stream_map)\n",
        "\n",
        "        statuses = global_user_df['Work status'].unique()\n",
        "        for i, status in enumerate(statuses):\n",
        "            subset = global_user_df[global_user_df['Work status'] == status]\n",
        "            ax2.scatter(\n",
        "                subset['Stream'].map(stream_map) + np.random.normal(0, 0.1, len(subset)),\n",
        "                subset['TotalYears'],\n",
        "                label=status, alpha=0.6, edgecolors='w'\n",
        "            )\n",
        "        ax2.set_xticks(range(len(streams)))\n",
        "        ax2.set_xticklabels(streams)\n",
        "        ax2.set_title('Distribution of Experience by Domain (Colored by Work Status)')\n",
        "        ax2.legend(title=\"Status\")\n",
        "        ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "        # Chart 3: Qualification\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        qual_counts = global_user_df['Highest Qualification'].value_counts().head(7).sort_values()\n",
        "        ax3.barh(qual_counts.index, qual_counts.values, color='mediumpurple')\n",
        "        ax3.set_title('Highest Qualification')\n",
        "\n",
        "        # Chart 4: Gender\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        gender_counts = global_user_df['gender'].value_counts()\n",
        "        ax4.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%',\n",
        "               colors=['lightblue', 'lightpink', 'lightgrey'], startangle=90)\n",
        "        ax4.set_title('Gender')\n",
        "\n",
        "        # Chart 5: Goals\n",
        "        ax5 = fig.add_subplot(gs[1, 2])\n",
        "        job_goal_counts = global_user_df['Job goal 1'].value_counts().head(5)\n",
        "        ax5.barh(job_goal_counts.index, job_goal_counts.values, color='gold')\n",
        "        ax5.set_title('Top 5 Job Goals')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating global dashboard: {e}\")\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.text(0.5, 0.5, f\"Error generating charts: {e}\", ha='center')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# --- 8. Submit Function ---\n",
        "async def process_new_user_submission(*args):\n",
        "    print(\"New user submitted. Processing...\")\n",
        "    if ai_models[\"pipeline\"] is None:\n",
        "        return \"ERROR: AI Models not loaded.\", \"Error\", None, \"Error\", None, None\n",
        "\n",
        "    new_profile_dict = dict(zip(ALL_FEATURE_COLUMNS, args))\n",
        "\n",
        "    try:\n",
        "        save_data = pd.DataFrame([new_profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "        save_data['Journey id'] = f\"WEB-{int(datetime.datetime.now().timestamp())}\"\n",
        "        save_data['Attachment date'] = datetime.date.today().isoformat()\n",
        "        save_data['Stream'] = 'Online Submission'\n",
        "        save_data['Work status'] = 'On programme'\n",
        "        file_exists = os.path.isfile(NEW_SUBMISSIONS_FILE_PATH)\n",
        "        save_data.to_csv(NEW_SUBMISSIONS_FILE_PATH, mode='a', header=not file_exists, index=False)\n",
        "        print(\"Profile saved to new_submissions.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- ERROR Saving to Drive: {e} ---\")\n",
        "\n",
        "    statistical_report, top_weakness, hire_probability = run_xgb_prediction_and_get_report(new_profile_dict)\n",
        "    individual_skill_chart = generate_individual_charts(new_profile_dict)\n",
        "\n",
        "    years = new_profile_dict.get('Years workexperience', 0)\n",
        "    total_years = float(years) + (float(new_profile_dict.get('Months workexperience', 0)) / 12)\n",
        "    if total_years <= 2: level = 'Junior'\n",
        "    elif total_years <= 5: level = 'Mid-level'\n",
        "    else: level = 'Senior'\n",
        "\n",
        "    categorization_text = f\"**Profile Category:**\\n\"\n",
        "    categorization_text += f\"  - **Level:** {level}\\n\"\n",
        "    categorization_text += f\"  - **Qualification:** {new_profile_dict.get('Highest Qualification', 'N/A')}\\n\"\n",
        "    categorization_text += f\"  - **Top Job Goal:** {new_profile_dict.get('Job goal 1', 'N/A')}\"\n",
        "\n",
        "    print(\"AI Counselor: Calling Gemini for RAG Action Plan...\")\n",
        "    job_goal = new_profile_dict.get('Job goal 1', 'N/A')\n",
        "    major = new_profile_dict.get('Highest major', 'N/A')\n",
        "\n",
        "    search_query_1 = f\"free online courses for {job_goal} with {major} background\"\n",
        "    search_query_2 = f\"best paid certificate programs on Coursera or Udemy for {job_goal}\"\n",
        "    search_query_3 = f\"professional networking groups or meetups for {job_goal} in Saudi Arabia\"\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an expert AI Career Counselor. A candidate has just completed their profile.\n",
        "    Your job is to write a comprehensive, encouraging, and actionable career plan.\n",
        "\n",
        "    HERE IS THE DATA YOU MUST USE:\n",
        "    1. THE CANDIDATE'S PROFILE: {json.dumps(new_profile_dict, indent=2)}\n",
        "    2. THE STATISTICAL ANALYSIS REPORT (from our predictive model): {statistical_report}\n",
        "    3. A LIST OF *INTERNAL* WORKSHOPS:\n",
        "       - \"Work ethics and an introduction to labor culture\"\n",
        "       - \"Initial appointment / Induction\", \"CV Writing\", \"Interview Skills\"\n",
        "\n",
        "    YOUR TASK:\n",
        "    1.  **Analyze:** Read the STATISTICAL REPORT. Identify the candidate's hire probability and their biggest weakness.\n",
        "    2.  **Search:** **USE THE GOOGLE SEARCH TOOL** to find real-world resources. You will perform 3 searches:\n",
        "        * Search 1: `{search_query_1}` (for free courses)\n",
        "        * Search 2: `{search_query_2}` (for paid certificates)\n",
        "        * Search 3: `{search_query_3}` (for meetups/networking)\n",
        "    4.  **Synthesize Plan:** Write a comprehensive, multi-part career plan.\n",
        "        * **Introduction:** Start by being encouraging.\n",
        "        * **Part 1: Immediate Profile Improvement (The Weakness):** Use the STATISTICAL REPORT to explain their *biggest statistical weakness*.\n",
        "        * **Part 2: Internal Workshops:** Recommend one *internal workshop* from the list.\n",
        "        * **Part 3: External Training Plan (Free):** Recommend 1-2 **free courses** you found from Search 1. **You MUST include the source title and the URL.**\n",
        "        * **Part 4: External Training Plan (Paid/Certificate):** Recommend 1-2 **paid certificate programs** from Search 2. **You MUST include the source title and the URL.**\n",
        "        * **Part 5: Community & Networking:** Recommend 1-2 **networking groups or meetups** from Search 3. **You MUST include the source title and the URL.**\n",
        "        * **Conclusion:** End with an encouraging closing statement.\n",
        "    \"\"\"\n",
        "\n",
        "    apiKey = MY_GOOGLE_API_KEY\n",
        "    apiUrl = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": \"Generate the career plan.\"}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\"responseMimeType\": \"text/plain\"},\n",
        "        \"tools\": [{\"google_search\": {}}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        transport = httpx.AsyncHTTPTransport(retries=3)\n",
        "        async with httpx.AsyncClient(transport=transport) as client:\n",
        "            response = await client.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'}, timeout=90.0)\n",
        "\n",
        "        if not response.status_code == 200:\n",
        "            raise Exception(f\"RAG API call failed: {response.text}\")\n",
        "\n",
        "        result = response.json()\n",
        "        if not result.get('candidates'): raise Exception(\"Invalid RAG response\")\n",
        "\n",
        "        final_action_plan = result['candidates'][0]['content']['parts'][0]['text']\n",
        "        print(\"...RAG Action Plan generated.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR Calling RAG API: {e} !!!---\")\n",
        "        final_action_plan = f\"An error occurred while generating the RAG action plan: {e}\"\n",
        "\n",
        "    updated_dashboard = generate_global_dashboard()\n",
        "\n",
        "    return (\n",
        "        f\"{hire_probability:.1%}\",\n",
        "        categorization_text,\n",
        "        individual_skill_chart,\n",
        "        final_action_plan,\n",
        "        final_action_plan,\n",
        "        updated_dashboard\n",
        "    )\n",
        "\n",
        "# --- 9. Follow-up Chat Function ---\n",
        "async def call_gemini_follow_up_chat(user_message: str, chat_history: list, report_context: str):\n",
        "    chat_history.append([user_message, None])\n",
        "    if not report_context:\n",
        "        chat_history[-1][1] = \"I'm sorry, you must generate a report in the 'New Candidate Report' tab first.\"\n",
        "        return \"\", chat_history\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an AI Career Counselor. Context:\n",
        "    {report_context}\n",
        "    User question: \"{user_message}\"\n",
        "    Answer the question using the report context and Google Search if needed.\n",
        "    \"\"\"\n",
        "\n",
        "    apiKey = MY_GOOGLE_API_KEY\n",
        "    apiUrl = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_message}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\"responseMimeType\": \"text/plain\"},\n",
        "        \"tools\": [{\"google_search\": {}}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        transport = httpx.AsyncHTTPTransport(retries=3)\n",
        "        async with httpx.AsyncClient(transport=transport) as client:\n",
        "            response = await client.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'}, timeout=90.0)\n",
        "        if not response.status_code == 200: raise Exception(f\"API failed: {response.text}\")\n",
        "        result = response.json()\n",
        "        bot_message = result['candidates'][0]['content']['parts'][0]['text']\n",
        "    except Exception as e:\n",
        "        bot_message = f\"Error: {e}\"\n",
        "\n",
        "    chat_history[-1][1] = bot_message\n",
        "    return \"\", chat_history\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- END OF LOGIC / START OF FULL PAGE UI (MULTI-PAGE TABS) ---\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "print(\"--- Phase 5: Launching the Gradio MULTI-PAGE App ---\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# AI-Powered Predictive Hiring & Action Intelligence System\")\n",
        "\n",
        "    report_context_state = gr.State(value=\"\")\n",
        "    chat_history_state = gr.State(value=[])\n",
        "\n",
        "    # --- TOP LEVEL TABS ---\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # --- PAGE 1: AI COUNSELOR ---\n",
        "        with gr.TabItem(\"ðŸ¤– AI Counselor & Prediction\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=3):\n",
        "                    gr.Markdown(\"### 1. Candidate Profile Input\")\n",
        "                    all_inputs = []\n",
        "                    with gr.Accordion(\"Personal Info\", open=True):\n",
        "                        with gr.Row():\n",
        "                            gender_input = gr.Dropdown(label=\"Gender\", choices=GENDER_CHOICES, value='Female')\n",
        "                            age_input = gr.Number(label=\"Age\", value=25)\n",
        "                        marital_input = gr.Dropdown(label=\"Marital Status\", choices=MARITAL_CHOICES, value='Single')\n",
        "                        all_inputs.extend([gender_input, age_input, marital_input])\n",
        "\n",
        "                    with gr.Accordion(\"Location\", open=False):\n",
        "                        town_input = gr.Textbox(label=\"Town\", value='Riyadh')\n",
        "                        district_input = gr.Textbox(label=\"District\", value='Central')\n",
        "                        all_inputs.extend([town_input, district_input])\n",
        "\n",
        "                    with gr.Accordion(\"Education & Salary\", open=False):\n",
        "                        salary_input = gr.Dropdown(label=\"Salary Expectations\", choices=SALARY_CHOICES, value='3,000 - 5,000SAR')\n",
        "                        qual_input = gr.Dropdown(label=\"Highest Qualification\", choices=QUALIFICATION_CHOICES, value='Bachelor')\n",
        "                        major_input = gr.Textbox(label=\"Highest Major\", value='Computer Science')\n",
        "                        all_inputs.extend([salary_input, qual_input, major_input])\n",
        "\n",
        "                    with gr.Accordion(\"Work Preferences\", open=False):\n",
        "                        shifts_input = gr.Dropdown(label=\"Shifts\", choices=SHIFTS_CHOICES, value='No preference')\n",
        "                        env_input = gr.Dropdown(label=\"Environment\", choices=ENVIRONMENT_CHOICES, value='Mixed')\n",
        "                        all_inputs.extend([shifts_input, env_input])\n",
        "\n",
        "                    with gr.Accordion(\"Skills\", open=False):\n",
        "                        eng_input = gr.Dropdown(label=\"English\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        word_input = gr.Dropdown(label=\"Word\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        excel_input = gr.Dropdown(label=\"Excel\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        ppt_input = gr.Dropdown(label=\"PPT\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        comm_input = gr.Dropdown(label=\"Communication\", choices=COMMUNICATION_CHOICES, value='Good')\n",
        "                        all_inputs.extend([eng_input, word_input, excel_input, ppt_input])\n",
        "\n",
        "                    with gr.Accordion(\"Logistics\", open=False):\n",
        "                        transport_input = gr.Radio(label=\"Transport?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        license_input = gr.Radio(label=\"License?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        cv_input = gr.Radio(label=\"CV?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        all_inputs.extend([transport_input, license_input])\n",
        "\n",
        "                    with gr.Accordion(\"Career Goals\", open=False):\n",
        "                        goal1_input = gr.Textbox(label=\"Job Goal 1\", value='Data Analyst')\n",
        "                        goal2_input = gr.Textbox(label=\"Job Goal 2\", value='Developer')\n",
        "                        goal3_input = gr.Textbox(label=\"Job Goal 3\", value='IT Support')\n",
        "                        all_inputs.extend([goal1_input, goal2_input, goal3_input])\n",
        "\n",
        "                    with gr.Accordion(\"Experience\", open=False):\n",
        "                        with gr.Row():\n",
        "                            years_exp_input = gr.Number(label=\"Years\", value=1)\n",
        "                            months_exp_input = gr.Number(label=\"Months\", value=0)\n",
        "                        all_inputs.extend([years_exp_input, months_exp_input])\n",
        "\n",
        "                    all_inputs.append(comm_input)\n",
        "                    all_inputs.append(cv_input)\n",
        "\n",
        "                    submit_btn = gr.Button(\"Generate AI Report\", variant=\"primary\")\n",
        "                    status_output = gr.Textbox(label=\"Status\", interactive=False, value=\"App starting...\")\n",
        "\n",
        "                with gr.Column(scale=7):\n",
        "                    gr.Markdown(\"### 2. AI Command Center\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        prediction_output = gr.Textbox(label=\"Hire Probability\", value=\"N/A\", scale=1)\n",
        "                        category_output = gr.Markdown(value=\"*Categorization*\") # Removed 'scale' arg\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=1):\n",
        "                            skill_chart_output = gr.Plot(label=\"Skills Radar\")\n",
        "                        with gr.Column(scale=2):\n",
        "                            action_plan_output = gr.Markdown(value=\"*Action Plan will appear here*\", label=\"AI Action Plan\")\n",
        "\n",
        "                    gr.Markdown(\"### 3. AI Counselor Chat\")\n",
        "                    chat_window = gr.Chatbot(label=\"Ask follow-up questions about the plan...\", height=300)\n",
        "                    with gr.Row():\n",
        "                        chat_textbox = gr.Textbox(show_label=False, placeholder=\"Ask me anything about the report above...\", scale=8)\n",
        "                        chat_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "        # --- PAGE 2: DASHBOARD ---\n",
        "        with gr.TabItem(\"ðŸ“Š Executive Analytics Dashboard\"):\n",
        "            gr.Markdown(\"### Global User Population Analysis\")\n",
        "            global_dashboard_plot = gr.Plot(label=\"Global Dashboard\")\n",
        "            refresh_button = gr.Button(\"Refresh Global Dashboard\", variant=\"secondary\")\n",
        "\n",
        "\n",
        "    # --- CONNECTING FUNCTIONS ---\n",
        "    demo.load(fn=on_app_load, outputs=[status_output]).then(fn=generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_new_user_submission,\n",
        "        inputs=all_inputs,\n",
        "        outputs=[prediction_output, category_output, skill_chart_output, action_plan_output, report_context_state, global_dashboard_plot]\n",
        "    )\n",
        "\n",
        "    refresh_button.click(fn=generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "\n",
        "    chat_btn.click(fn=call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "    chat_textbox.submit(fn=call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "\n",
        "print(\"Launching...\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K953fvxplKX8",
        "outputId": "9db949b9-4019-430e-b2c8-9b177d52629c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\n",
            "Successfully loaded Google API Key from Colab Secrets.\n",
            "--- Phase 5: Launching the Gradio MULTI-PAGE App ---\n",
            "Launching...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6c4838b7db37cbe914.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6c4838b7db37cbe914.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\n",
            "...XGBoost Models loaded successfully.\n",
            "...Loaded 2000 users for Global Dashboard.\n",
            "Generating global dashboard...\n",
            "Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\n",
            "...XGBoost Models loaded successfully.\n",
            "...Loaded 2000 users for Global Dashboard.\n",
            "New user submitted. Processing...\n",
            "Profile saved to new_submissions.csv\n",
            "XGBoost Brain: Running prediction...\n",
            "...XGBoost Brain: Report generated. Top weakness: Highest Qualification\n",
            "Generating individual skills radar chart...\n",
            "AI Counselor: Calling Gemini for RAG Action Plan...\n",
            "...RAG Action Plan generated.\n",
            "Generating global dashboard...\n",
            "New user submitted. Processing...\n",
            "Profile saved to new_submissions.csv\n",
            "XGBoost Brain: Running prediction...\n",
            "...XGBoost Brain: Report generated. Top weakness: Transport available\n",
            "Generating individual skills radar chart...\n",
            "AI Counselor: Calling Gemini for RAG Action Plan...\n",
            "...RAG Action Plan generated.\n",
            "Generating global dashboard...\n",
            "Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\n",
            "...XGBoost Models loaded successfully.\n",
            "...Loaded 2000 users for Global Dashboard.\n",
            "Generating global dashboard...\n",
            "Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\n",
            "...XGBoost Models loaded successfully.\n",
            "...Loaded 2000 users for Global Dashboard.\n",
            "Generating global dashboard...\n",
            "Generating global dashboard...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://03f222cb8c3e851914.gradio.live\n",
            "Killing tunnel 127.0.0.1:7860 <> https://645bef6be74fb80c4a.gradio.live\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6c4838b7db37cbe914.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8"
      ],
      "metadata": {
        "id": "JhkC_YkFPpJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9"
      ],
      "metadata": {
        "id": "6K-luV5yPuQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10"
      ],
      "metadata": {
        "id": "ylmnRy7SPwoH"
      }
    }
  ]
}