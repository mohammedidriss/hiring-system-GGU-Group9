{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkA6CoVZ4CGcMQJr+2yBY7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedidriss/hiring-system-GGU-Group9/blob/master/Course4_Hiring_system_v1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1"
      ],
      "metadata": {
        "id": "w5cv58GEPa7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkXBCetjbMSw",
        "outputId": "3938457e-a928-4861-c730-df69f87fa4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries: gradio, xgboost, shap, openpyxl, httpx...\n",
            "Installations complete.\n"
          ]
        }
      ],
      "source": [
        "# This cell installs all the external libraries\n",
        "print(\"Installing required libraries: gradio, xgboost, shap, openpyxl, httpx...\")\n",
        "!pip install -q gradio xgboost shap openpyxl httpx\n",
        "print(\"Installations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2"
      ],
      "metadata": {
        "id": "Wf2JtT7zPfLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports all the tools.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "import joblib\n",
        "import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import shap\n",
        "\n",
        "import httpx\n",
        "import asyncio\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported. Ready to mount Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf3tmiZOmjgv",
        "outputId": "59f38482-e9d9-43a7-ae85-c7cee4728e68"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported. Ready to mount Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3"
      ],
      "metadata": {
        "id": "60jpIQ6FPg2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connects  Colab notebook to Google Drive\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully at /content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyFxY6-Lmlsw",
        "outputId": "75ac1ed8-5716-42dd-c866-783bccdd7309"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4"
      ],
      "metadata": {
        "id": "FCUyHL3WPibz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell defines all our file paths and creates the \"Master Dataset.\"\n",
        "# It reads your two Excel sheets (from the same file), merges them,\n",
        "# creates the 'TARGET_HIRED' label, and saves this new\n",
        "# \"master\" file back to your Drive.\n",
        "\n",
        "print(\"--- Phase 1: Creating Master Training Dataset ---\")\n",
        "\n",
        "# --- 1. Define File Paths ---\n",
        "# This is our original Excel file\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
        "FEATURES_FILE_PATH = f\"{DRIVE_PATH}SampleData.xlsx\"\n",
        "FEATURES_SHEET_NAME = \"sample_jsdrilldown\"\n",
        "OUTCOMES_SHEET_NAME = \"Sample_Workshop_app\" # The other sheet in your file\n",
        "MASTER_DATASET_PATH = f\"{DRIVE_PATH}master_training_dataset.csv\"\n",
        "\n",
        "# --- 2. Define the Function ---\n",
        "def create_master_dataset():\n",
        "    \"\"\"\n",
        "    Merges features and outcomes, calculates target labels,\n",
        "    and saves a new \"master\" dataset.\n",
        "    \"\"\"\n",
        "    print(f\"Loading features from: {FEATURES_SHEET_NAME}\")\n",
        "    try:\n",
        "        df_features = pd.read_excel(FEATURES_FILE_PATH, sheet_name=FEATURES_SHEET_NAME)\n",
        "        print(f\"Loaded {len(df_features)} feature rows.\")\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR loading features sheet: {e} !!!---\")\n",
        "        print(\"Please check: 1. File path is correct. 2. Sheet name is correct. 3. 'openpyxl' is installed.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading outcomes from: {OUTCOMES_SHEET_NAME}\")\n",
        "    try:\n",
        "        # We only need the 'Journey id' and the 'Status' columns\n",
        "        df_outcomes = pd.read_excel(FEATURES_FILE_PATH, sheet_name=OUTCOMES_SHEET_NAME, usecols=['Journey id', 'Status'])\n",
        "        print(f\"Loaded {len(df_outcomes)} outcome rows.\")\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR loading outcomes sheet: {e} !!!---\")\n",
        "        return None\n",
        "\n",
        "    # --- 3. Create the Classification Label (The \"Prediction\") ---\n",
        "    # Use 'Status' == 'In work' as our \"Hired\" (1) or \"Not Hired\" (0) label\n",
        "    df_outcomes_clean = df_outcomes.drop_duplicates(subset=['Journey id'])\n",
        "    df_outcomes_clean['TARGET_HIRED'] = np.where(df_outcomes_clean['Status'] == 'In work', 1, 0)\n",
        "\n",
        "    # --- 4. Merge Features and Labels ---\n",
        "    # We use a 'left' merge to keep all candidates from the features sheet\n",
        "    # and add the 'TARGET_HIRED' label to them.\n",
        "    #\n",
        "    print(\"Merging features and labels...\")\n",
        "    df_master = pd.merge(\n",
        "        df_features,\n",
        "        df_outcomes_clean[['Journey id', 'TARGET_HIRED']],\n",
        "        on='Journey id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Fill any candidate who has no outcome data with 0 (Not Hired)\n",
        "    df_master['TARGET_HIRED'] = df_master['TARGET_HIRED'].fillna(0).astype(int)\n",
        "\n",
        "    print(f\"Master dataset created with {len(df_master)} rows.\")\n",
        "\n",
        "    # --- 5. Save the new master file ---\n",
        "    df_master.to_csv(MASTER_DATASET_PATH, index=False)\n",
        "    print(f\"Master dataset saved to: {MASTER_DATASET_PATH}\")\n",
        "    return df_master\n",
        "\n",
        "# --- 6. RUN THE FUNCTION ---\n",
        "master_df = create_master_dataset()\n",
        "if master_df is not None:\n",
        "    print(f\"We have {master_df['TARGET_HIRED'].sum()} 'Hired' users to train on.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs_qIuufmxhi",
        "outputId": "1b39db02-8d52-4c0c-9b6d-8d9340240e62"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Creating Master Training Dataset ---\n",
            "Loading features from: sample_jsdrilldown\n",
            "Loaded 2000 feature rows.\n",
            "Loading outcomes from: Sample_Workshop_app\n",
            "Loaded 5375 outcome rows.\n",
            "Merging features and labels...\n",
            "Master dataset created with 2000 rows.\n",
            "Master dataset saved to: /content/drive/MyDrive/master_training_dataset.csv\n",
            "We have 1000 'Hired' users to train on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5"
      ],
      "metadata": {
        "id": "Oc9We-v6Pj8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell re-builds the \"AI Brain\" (XGBoost Model).\n",
        "# FIX: Added text lowercasing and regularization to prevent 0% predictions.\n",
        "\n",
        "print(\"--- Phase 2: Building the AI Brain (Robust XGBoost) ---\")\n",
        "\n",
        "# Define the brain file path here, similar to MASTER_DATASET_PATH\n",
        "BRAIN_FILE_PATH = f\"{DRIVE_PATH}ai_brain_pipeline.joblib\"\n",
        "\n",
        "if 'master_df' not in locals() or master_df.empty:\n",
        "    print(\"ERROR: 'master_df' not found. Please run Cell 4 first.\")\n",
        "else:\n",
        "    TARGET_COLUMN = 'TARGET_HIRED'\n",
        "\n",
        "    # 1. Define Features (Strict List - No Leakage)\n",
        "    AI_FEATURE_COLUMNS = [\n",
        "        'gender', 'Age', 'Marital status', 'JS Town', 'JS Town distrinct',\n",
        "        'Salary expectations', 'Highest Qualification', 'Highest major',\n",
        "        'Shifts', 'Working environment', 'Skills English', 'Skills MS Word',\n",
        "        'Skills MS Excel', 'Skills MS Powerpoint', 'Transport available', 'Driving license',\n",
        "        'Job goal 1', 'Job goal 2', 'Job goal 3', 'Years workexperience',\n",
        "        'Months workexperience', 'Communication skills', 'Has CV (SWS)'\n",
        "    ]\n",
        "\n",
        "    # 2. Standardize Text (Lowercasing) - CRITICAL FIX\n",
        "    # We create a copy to avoid warnings\n",
        "    training_df = master_df.copy()\n",
        "\n",
        "    # Convert all object/string columns to lowercase\n",
        "    # This ensures \"Data Analyst\" matches \"data analyst\"\n",
        "    for col in AI_FEATURE_COLUMNS:\n",
        "        if training_df[col].dtype == 'object':\n",
        "            training_df[col] = training_df[col].astype(str).str.lower()\n",
        "\n",
        "    # 3. Separate Features\n",
        "    numeric_features = training_df[AI_FEATURE_COLUMNS].select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = training_df[AI_FEATURE_COLUMNS].select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "    ordinal_features = ['Skills English', 'Skills MS Word', 'Skills MS Excel', 'Skills MS Powerpoint', 'Communication skills']\n",
        "    skill_levels = ['n_a', 'beginner', 'good', 'excellent'] # Lowercase now!\n",
        "\n",
        "    categorical_features = [col for col in categorical_features if col not in ordinal_features]\n",
        "\n",
        "    print(f\"Training on {len(AI_FEATURE_COLUMNS)} features.\")\n",
        "\n",
        "    # 4. Build Pipelines\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    ordinal_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='n_a')),\n",
        "        ('encoder', OrdinalEncoder(categories=[skill_levels] * len(ordinal_features), handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "    ])\n",
        "\n",
        "    # Use OneHot with 'ignore' to handle unseen job titles gracefully\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('ord', ordinal_transformer, ordinal_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # 5. Create Regularized XGBoost Pipeline\n",
        "    ai_brain_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', xgb.XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False,\n",
        "            scale_pos_weight=1,\n",
        "            random_state=42,\n",
        "            # --- REGULARIZATION (The \"Fuzzy\" Fix) ---\n",
        "            max_depth=3,          # Shallower trees prevent memorization\n",
        "            learning_rate=0.05,   # Slower learning is more robust\n",
        "            n_estimators=100,\n",
        "            subsample=0.8,        # Use only 80% of data per tree (adds noise)\n",
        "            colsample_bytree=0.8  # Use only 80% of features per tree\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 6. Train\n",
        "    print(\"\\nSplitting data and training model...\")\n",
        "    X = training_df[AI_FEATURE_COLUMNS]\n",
        "    y = training_df[TARGET_COLUMN]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    ai_brain_pipeline.fit(X_train, y_train)\n",
        "    print(\"...Training complete.\")\n",
        "\n",
        "    # 7. Evaluate & Feature Importance Check\n",
        "    print(\"\\n--- Model Performance ---\")\n",
        "    preds = ai_brain_pipeline.predict(X_test)\n",
        "    probs = ai_brain_pipeline.predict_proba(X_test)[:, 1]\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, preds):.2%}\")\n",
        "    print(f\"Avg Prediction Probability: {probs.mean():.2%}\") # Should be around 50% for balanced data\n",
        "\n",
        "    # Check Feature Importance to spot Leakage\n",
        "    print(\"\\n--- TOP 5 DRIVERS OF HIRING (Check for Leakage) ---\")\n",
        "    # Extract feature names\n",
        "    try:\n",
        "        ohe_feature_names = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
        "        all_names = numeric_features + ordinal_features + list(ohe_feature_names)\n",
        "        importances = ai_brain_pipeline.named_steps['classifier'].feature_importances_\n",
        "\n",
        "        feat_imp = pd.DataFrame({'Feature': all_names, 'Importance': importances})\n",
        "        feat_imp = feat_imp.sort_values(by='Importance', ascending=False).head(10)\n",
        "        print(feat_imp)\n",
        "    except:\n",
        "        print(\"Could not print feature importance (minor issue).\")\n",
        "\n",
        "    # 8. Save\n",
        "    joblib.dump(ai_brain_pipeline, BRAIN_FILE_PATH)\n",
        "    print(f\"\\nSUCCESS: Robust AI Brain saved to: {BRAIN_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFNzkhM-msur",
        "outputId": "1fa6c562-9f9d-40b9-e847-2d983950ac86"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 2: Building the AI Brain (Robust XGBoost) ---\n",
            "Training on 23 features.\n",
            "\n",
            "Splitting data and training model...\n",
            "...Training complete.\n",
            "\n",
            "--- Model Performance ---\n",
            "Accuracy: 65.75%\n",
            "Avg Prediction Probability: 50.62%\n",
            "\n",
            "--- TOP 5 DRIVERS OF HIRING (Check for Leakage) ---\n",
            "                                  Feature  Importance\n",
            "9                             gender_male    0.041823\n",
            "58         Highest Qualification_bachelor    0.040299\n",
            "23                         JS Town_dammam    0.036709\n",
            "8                           gender_female    0.028481\n",
            "122                     Highest major_nan    0.018478\n",
            "37                         JS Town_medina    0.018086\n",
            "35                         JS Town_khobar    0.017873\n",
            "170  Job goal 1_electricity gas and water    0.016765\n",
            "34                 JS Town_khamis mushait    0.016131\n",
            "146                  Shifts_no preference    0.016071\n",
            "\n",
            "SUCCESS: Robust AI Brain saved to: /content/drive/MyDrive/ai_brain_pipeline.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6"
      ],
      "metadata": {
        "id": "qcofwQ2GPlfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell builds the \"Action Plan Generator\" .\n",
        "\n",
        "\n",
        "print(\"--- Phase 3: Building the Action Plan Generator (SHAP) ---\")\n",
        "\n",
        "# --- 1. Load the Saved AI Brain ---\n",
        "# (BRAIN_FILE_PATH and EXPLAINER_FILE_PATH were defined in Cell 3)\n",
        "# FIX: Define EXPLAINER_FILE_PATH here\n",
        "EXPLAINER_FILE_PATH = f\"{DRIVE_PATH}shap_explainer.joblib\"\n",
        "\n",
        "try:\n",
        "    ai_brain_pipeline = joblib.load(BRAIN_FILE_PATH)\n",
        "    print(f\"Successfully loaded AI Brain from: {BRAIN_FILE_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR loading 'ai_brain_pipeline.joblib': {e} !!!---\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Separate Pipeline Components ---\n",
        "preprocessor = ai_brain_pipeline.named_steps['preprocessor']\n",
        "model = ai_brain_pipeline.named_steps['classifier']\n",
        "\n",
        "# --- 3. Get Feature Names (FIXED METHOD) ---\n",
        "# We ask the preprocessor for its *actual* output names\n",
        "print(\"Getting feature names directly from the preprocessor...\")\n",
        "try:\n",
        "    all_transformed_feature_names = preprocessor.get_feature_names_out().tolist()\n",
        "    print(f\"Successfully got {len(all_transformed_feature_names)} feature names.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting feature names: {e}.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. Transform Training Data (FIXED METHOD) ---\n",
        "print(\"Transforming training data for SHAP explainer...\")\n",
        "\n",
        "X = master_df[AI_FEATURE_COLUMNS]\n",
        "y = master_df[TARGET_COLUMN]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "print(\"...Data transformed successfully (dense array).\")\n",
        "\n",
        "# --- 5. Build the SHAP Explainer ---\n",
        "print(\"Building SHAP TreeExplainer...\")\n",
        "#\n",
        "explainer = shap.TreeExplainer(model, X_train_transformed)\n",
        "print(\"...Explainer built successfully.\")\n",
        "\n",
        "# --- 6. Save the SHAP Explainer ---\n",
        "joblib.dump(explainer, EXPLAINER_FILE_PATH)\n",
        "print(f\"SUCCESS: SHAP Explainer saved to: {EXPLAINER_FILE_PATH}\")\n",
        "\n",
        "# --- 7. Define Test Function ---\n",
        "def generate_action_plan_test(new_user_data_df):\n",
        "    prediction_proba = ai_brain_pipeline.predict_proba(new_user_data_df)[0]\n",
        "    hire_probability = prediction_proba[1]\n",
        "    prediction_raw = ai_brain_pipeline.predict(new_user_data_df)[0]\n",
        "\n",
        "    # We DON'T call .toarray() here\n",
        "    user_transformed = preprocessor.transform(new_user_data_df)\n",
        "    shap_values = explainer.shap_values(user_transformed)\n",
        "\n",
        "    # This will now work\n",
        "    df_shap = pd.DataFrame(shap_values, columns=all_transformed_feature_names).iloc[0].T\n",
        "    df_shap = df_shap.reset_index(); df_shap.columns = ['Feature', 'SHAP_Value']\n",
        "    df_shap['abs_impact'] = df_shap['SHAP_Value'].abs()\n",
        "    df_shap = df_shap.sort_values(by='abs_impact', ascending=False)\n",
        "\n",
        "    top_factors = df_shap.head(5)\n",
        "\n",
        "    action_plan = \"--- (TEST) Recommended Action Plan ---\\n\"\n",
        "    for _, row in top_factors.iterrows():\n",
        "        sign = \"[+]\" if row['SHAP_Value'] > 0 else \"[-]\"\n",
        "        action_plan += f\"  {sign} {row['Feature']} (Impact: {row['SHAP_Value']:.2f})\\n\"\n",
        "    action_plan += f\"\\n  Prediction: {'Hired' if prediction_raw == 1 else 'Not Hired'} ({hire_probability:.1%})\"\n",
        "    return action_plan\n",
        "\n",
        "# --- 8. Test the Explainer ---\n",
        "print(\"\\n--- TESTING THE ACTION PLAN GENERATOR ---\")\n",
        "# We use X_test, which we just created in this cell\n",
        "sample_user_df = X_test.iloc[0:1]\n",
        "true_label = y_test.iloc[0]\n",
        "print(f\"Generating plan for a sample user. (True Label: {'Hired' if true_label == 1 else 'Not Hired'})...\")\n",
        "print(generate_action_plan_test(sample_user_df))\n",
        "print(\"---------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J34l0WQlu-22",
        "outputId": "745e457f-70b3-4585-c49d-0eadf849b222"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 3: Building the Action Plan Generator (SHAP) ---\n",
            "Successfully loaded AI Brain from: /content/drive/MyDrive/ai_brain_pipeline.joblib\n",
            "Getting feature names directly from the preprocessor...\n",
            "Successfully got 297 feature names.\n",
            "Transforming training data for SHAP explainer...\n",
            "...Data transformed successfully (dense array).\n",
            "Building SHAP TreeExplainer...\n",
            "...Explainer built successfully.\n",
            "SUCCESS: SHAP Explainer saved to: /content/drive/MyDrive/shap_explainer.joblib\n",
            "\n",
            "--- TESTING THE ACTION PLAN GENERATOR ---\n",
            "Generating plan for a sample user. (True Label: Not Hired)...\n",
            "--- (TEST) Recommended Action Plan ---\n",
            "  [+] num__Age (Impact: 0.22)\n",
            "  [+] num__Years workexperience (Impact: 0.02)\n",
            "  [-] cat__Job goal 2_agriculture fishing and grazing horses (Impact: 0.00)\n",
            "  [-] cat__Job goal 2_agriculture and livestock production (Impact: 0.00)\n",
            "  [-] cat__Job goal 2_accommodation and tourism (Impact: 0.00)\n",
            "\n",
            "  Prediction: Hired (52.4%)\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7"
      ],
      "metadata": {
        "id": "A1f1Wm94Pnkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the FINAL, COMPLETE cell for the application.\n",
        "# It includes ALL requested features and the final Multi-Page UI.\n",
        "# FIX: Explicit feature alignment and type conversion to ensure non-zero predictions.\n",
        "\n",
        "import datetime\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import json\n",
        "import httpx\n",
        "import asyncio\n",
        "from google.colab import userdata\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "import os\n",
        "\n",
        "# --- Install pypdf for resume parsing ---\n",
        "try:\n",
        "    import pypdf\n",
        "except ImportError:\n",
        "    os.system('pip install -q pypdf')\n",
        "    import pypdf\n",
        "\n",
        "print(\"--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\")\n",
        "\n",
        "# --- 1. Load API Key from Colab Secrets ---\n",
        "try:\n",
        "    MY_GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    if not MY_GOOGLE_API_KEY:\n",
        "        raise ValueError(\"API Key is empty or not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR: API KEY NOT FOUND: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Global Variables ---\n",
        "ai_models = {\"pipeline\": None, \"explainer\": None, \"feature_names\": None}\n",
        "global_user_df = pd.DataFrame()\n",
        "\n",
        "# Define paths explicitly to avoid scope issues\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
        "BRAIN_FILE_PATH = f\"{DRIVE_PATH}ai_brain_pipeline.joblib\"\n",
        "EXPLAINER_FILE_PATH = f\"{DRIVE_PATH}ai_explainer.joblib\"\n",
        "MASTER_DATASET_PATH = f\"{DRIVE_PATH}master_training_dataset.csv\"\n",
        "NEW_SUBMISSIONS_FILE_PATH = f\"{DRIVE_PATH}new_submissions.csv\"\n",
        "\n",
        "# This list MUST match the list in Cell 5\n",
        "ALL_FEATURE_COLUMNS = [\n",
        "    'gender', 'Age', 'Marital status', 'JS Town', 'JS Town distrinct',\n",
        "    'Salary expectations', 'Highest Qualification', 'Highest major',\n",
        "    'Shifts', 'Working environment', 'Skills English', 'Skills MS Word',\n",
        "    'Skills MS Excel', 'Skills MS Powerpoint', 'Transport available', 'Driving license',\n",
        "    'Job goal 1', 'Job goal 2', 'Job goal 3', 'Years workexperience',\n",
        "    'Months workexperience', 'Communication skills', 'Has CV (SWS)'\n",
        "]\n",
        "\n",
        "# --- 3. Define Dropdown Choices for the UI ---\n",
        "GENDER_CHOICES = ['Male', 'Female', 'Other', 'Prefer not to say']\n",
        "MARITAL_CHOICES = ['Single', 'Married', 'Divorced', 'Widowed', 'Other']\n",
        "SALARY_CHOICES = ['< 3,000SAR', '3,000 - 5,000SAR', '5,000 - 7,000SAR', '> 7,000SAR']\n",
        "QUALIFICATION_CHOICES = ['High School', 'Diploma', 'Bachelor', 'Masters', 'Doctorate']\n",
        "SHIFTS_CHOICES = ['No preference', 'Straight shifts', 'Rotating shifts']\n",
        "ENVIRONMENT_CHOICES = ['Mixed', 'Flexible', 'On-site', 'Remote']\n",
        "SKILL_LEVEL_CHOICES = ['N_A', 'Beginner', 'Good', 'Excellent']\n",
        "YES_NO_CHOICES = ['Yes', 'No']\n",
        "COMMUNICATION_CHOICES = ['Beginner', 'Good', 'Excellent']\n",
        "\n",
        "\n",
        "# --- 4. Function to Load XGBoost AI Brain (Runs Once) ---\n",
        "def on_app_load():\n",
        "    global ai_models, global_user_df\n",
        "    print(\"Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\")\n",
        "    try:\n",
        "        ai_models[\"pipeline\"] = joblib.load(BRAIN_FILE_PATH)\n",
        "        ai_models[\"explainer\"] = joblib.load(EXPLAINER_FILE_PATH)\n",
        "        preprocessor = ai_models[\"pipeline\"].named_steps['preprocessor']\n",
        "        ai_models[\"feature_names\"] = preprocessor.get_feature_names_out().tolist()\n",
        "        print(\"...XGBoost Models loaded successfully.\")\n",
        "\n",
        "        # Load Global Data\n",
        "        if os.path.exists(MASTER_DATASET_PATH):\n",
        "            global_user_df = pd.read_csv(MASTER_DATASET_PATH)\n",
        "            print(f\"...Loaded {len(global_user_df)} users for Global Dashboard.\")\n",
        "        else:\n",
        "            print(\"Warning: Master dataset not found.\")\n",
        "\n",
        "        return \"System Ready. AI Models and Global Data loaded.\"\n",
        "    except Exception as e:\n",
        "        return f\"CRITICAL ERROR: Could not load AI modules or data. {e}\"\n",
        "\n",
        "# --- 5. Helper Functions ---\n",
        "def format_feature_name_for_llm(feature_name):\n",
        "    name = feature_name.replace(\"cat__\", \"\").replace(\"ord__\", \"\").replace(\"num__\", \"\")\n",
        "    parts = name.split('_', 1)\n",
        "    if len(parts) == 2:\n",
        "        if \"Skills\" in parts[0]: return parts[0].replace(\"Skills \", \"\")\n",
        "        if \"Job goal\" in parts[0]: return \"Job Goal Setting\"\n",
        "        return parts[0]\n",
        "    return name\n",
        "\n",
        "def run_xgb_prediction_and_get_report(profile_dict):\n",
        "    print(\"XGBoost Brain: Preparing prediction...\")\n",
        "    try:\n",
        "        if ai_models[\"pipeline\"] is None: return \"Error: XGBoost model not loaded.\", \"Unknown\", 0.0\n",
        "\n",
        "        # DEBUG: Print input to verify it's not empty\n",
        "        # print(f\"DEBUG Input Profile: {profile_dict}\")\n",
        "\n",
        "        new_user_df = pd.DataFrame([profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "\n",
        "        # Ensure numeric types are correct\n",
        "        numeric_cols = ['Age', 'Years workexperience', 'Months workexperience']\n",
        "        for col in numeric_cols:\n",
        "            new_user_df[col] = pd.to_numeric(new_user_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "        pipeline = ai_models[\"pipeline\"]\n",
        "        explainer = ai_models[\"explainer\"]\n",
        "        feature_names = ai_models[\"feature_names\"]\n",
        "\n",
        "        # Prediction\n",
        "        prediction_proba = pipeline.predict_proba(new_user_df)[0]\n",
        "        hire_probability = prediction_proba[1]\n",
        "        print(f\"DEBUG Prediction: {hire_probability}\")\n",
        "\n",
        "        # Explanation\n",
        "        user_transformed = pipeline.named_steps['preprocessor'].transform(new_user_df)\n",
        "        shap_values = explainer.shap_values(user_transformed)\n",
        "        df_shap = pd.DataFrame(shap_values, columns=feature_names).iloc[0].T\n",
        "        df_shap = df_shap.reset_index(); df_shap.columns = ['Feature', 'SHAP_Value']\n",
        "        df_shap['abs_impact'] = df_shap['SHAP_Value'].abs()\n",
        "\n",
        "        df_negatives = df_shap[df_shap['SHAP_Value'] < 0].sort_values(by='SHAP_Value', ascending=True)\n",
        "        top_weakness_topic = \"General Profile Improvement\"\n",
        "        if not df_negatives.empty:\n",
        "            top_weakness_topic = format_feature_name_for_llm(df_negatives.iloc[0]['Feature'])\n",
        "\n",
        "        report = f\"STATISTICAL ANALYSIS REPORT:\\nPredicted Hire Probability: {hire_probability:.1%}\\nTop 5 Factors:\\n\"\n",
        "        for _, row in df_shap.sort_values(by='abs_impact', ascending=False).head(5).iterrows():\n",
        "            sign = \"POSITIVE\" if row['SHAP_Value'] > 0 else \"NEGATIVE\"\n",
        "            report += f\"  - Factor: {row['Feature']}, Impact: {sign}\\n\"\n",
        "\n",
        "        return report, top_weakness_topic, hire_probability\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction Error: {e}\")\n",
        "        return f\"Error during XGBoost prediction: {e}\", \"Unknown\", 0.0\n",
        "\n",
        "# --- 6. Resume Parsing Logic ---\n",
        "async def parse_resume(file_obj, *current_inputs):\n",
        "    if file_obj is None: return list(current_inputs)\n",
        "    try:\n",
        "        pdf_reader = pypdf.PdfReader(file_obj.name)\n",
        "        resume_text = \"\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "        prompt = f\"\"\"\n",
        "        Extract data from RESUME TEXT to JSON.\n",
        "        RESUME TEXT: {resume_text[:10000]}\n",
        "        REQUIRED KEYS: {ALL_FEATURE_COLUMNS}\n",
        "        Use \"Unknown\" or 0 if missing.\n",
        "        \"\"\"\n",
        "        json_str = await call_gemini_api(prompt, json_mode=True)\n",
        "        parsed_data = json.loads(json_str)\n",
        "        updates = []\n",
        "        for col in ALL_FEATURE_COLUMNS:\n",
        "            val = parsed_data.get(col, None)\n",
        "            updates.append(gr.update(value=val) if val is not None else gr.update())\n",
        "        return updates\n",
        "    except Exception as e:\n",
        "        print(f\"Resume parsing error: {e}\")\n",
        "        return list(current_inputs)\n",
        "\n",
        "# --- 7. Chart Generation Functions ---\n",
        "def generate_individual_charts(profile_dict):\n",
        "    skills = {k: profile_dict.get(f'Skills {k}', 'N_A') for k in ['English', 'MS Word', 'MS Excel', 'MS Powerpoint']}\n",
        "    skills['Communication'] = profile_dict.get('Communication skills', 'N_A')\n",
        "    level_map = {'N_A': 0, 'Beginner': 1, 'Good': 2, 'Excellent': 3}\n",
        "    values = [level_map.get(v, 0) for v in skills.values()]\n",
        "    labels = list(skills.keys())\n",
        "    N = len(labels); angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
        "    values += values[:1]; angles += angles[:1]\n",
        "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
        "    ax.fill(angles, values, color='blue', alpha=0.25); ax.plot(angles, values, color='blue', linewidth=2)\n",
        "    ax.set_xticks(angles[:-1]); ax.set_xticklabels(labels); plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def generate_global_dashboard():\n",
        "    global global_user_df\n",
        "    if global_user_df.empty:\n",
        "        fig, ax = plt.subplots(); ax.text(0.5, 0.5, \"Global user data not loaded.\", ha='center'); return fig\n",
        "    fig = plt.figure(figsize=(16, 12)); gs = fig.add_gridspec(2, 3)\n",
        "    try:\n",
        "        # 1. Domain\n",
        "        ax1 = fig.add_subplot(gs[0, 0]);\n",
        "        if 'Stream' in global_user_df.columns:\n",
        "            domain_counts = global_user_df['Stream'].value_counts()\n",
        "            ax1.bar(domain_counts.index, domain_counts.values, color='teal'); ax1.set_title('Candidates by Domain'); ax1.tick_params(axis='x', rotation=25)\n",
        "        # 2. Scatter\n",
        "        ax2 = fig.add_subplot(gs[0, 1:]);\n",
        "        years = pd.to_numeric(global_user_df['Years workexperience'].fillna(0), errors='coerce').fillna(0)\n",
        "        months = pd.to_numeric(global_user_df['Months workexperience'].fillna(0), errors='coerce').fillna(0)\n",
        "        global_user_df['TotalYears'] = years + (months / 12)\n",
        "        if 'Stream' in global_user_df.columns and 'Work status' in global_user_df.columns:\n",
        "            streams = global_user_df['Stream'].dropna().unique(); stream_map = {stream: i for i, stream in enumerate(streams)}\n",
        "            for status in global_user_df['Work status'].unique():\n",
        "                subset = global_user_df[global_user_df['Work status'] == status]\n",
        "                ax2.scatter(subset['Stream'].map(stream_map) + np.random.normal(0, 0.1, len(subset)), subset['TotalYears'], label=status, alpha=0.6, edgecolors='w')\n",
        "            ax2.set_xticks(range(len(streams))); ax2.set_xticklabels(streams); ax2.legend(title=\"Status\")\n",
        "        # 3. Qualification\n",
        "        ax3 = fig.add_subplot(gs[1, 0]);\n",
        "        if 'Highest Qualification' in global_user_df.columns:\n",
        "            global_user_df['Highest Qualification'].value_counts().head(7).sort_values().plot(kind='barh', ax=ax3, color='purple', title='Qualifications')\n",
        "        # 4. Gender\n",
        "        ax4 = fig.add_subplot(gs[1, 1]);\n",
        "        if 'gender' in global_user_df.columns:\n",
        "            global_user_df['gender'].value_counts().plot(kind='pie', ax=ax4, autopct='%1.1f%%', title='Gender')\n",
        "        # 5. Goals\n",
        "        ax5 = fig.add_subplot(gs[1, 2]);\n",
        "        if 'Job goal 1' in global_user_df.columns:\n",
        "            global_user_df['Job goal 1'].value_counts().head(5).plot(kind='barh', ax=ax5, color='gold', title='Top Goals')\n",
        "        plt.tight_layout()\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.text(0.5, 0.5, f\"Chart Error: {e}\", ha='center')\n",
        "    return fig\n",
        "\n",
        "# --- 8. Submit Function ---\n",
        "async def process_new_user_submission(*args):\n",
        "    if ai_models[\"pipeline\"] is None: return \"ERROR: AI Models not loaded.\", \"Error\", None, \"Error\", None, None, None\n",
        "\n",
        "    new_profile_dict = dict(zip(ALL_FEATURE_COLUMNS, args))\n",
        "\n",
        "    # Save data\n",
        "    try:\n",
        "        save_data = pd.DataFrame([new_profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "        save_data['Journey id'] = f\"WEB-{int(datetime.datetime.now().timestamp())}\"\n",
        "        save_data['Attachment date'] = datetime.date.today().isoformat()\n",
        "        save_data['Stream'] = 'Online Submission'; save_data['Work status'] = 'On programme'\n",
        "        file_exists = os.path.isfile(NEW_SUBMISSIONS_FILE_PATH)\n",
        "        save_data.to_csv(NEW_SUBMISSIONS_FILE_PATH, mode='a', header=not file_exists, index=False)\n",
        "        print(\"Profile saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- ERROR Saving: {e} ---\")\n",
        "\n",
        "    statistical_report, top_weakness, hire_probability = run_xgb_prediction_and_get_report(new_profile_dict)\n",
        "    individual_skill_chart = generate_individual_charts(new_profile_dict)\n",
        "\n",
        "    years = pd.to_numeric(new_profile_dict.get('Years workexperience', 0), errors='coerce')\n",
        "    months = pd.to_numeric(new_profile_dict.get('Months workexperience', 0), errors='coerce')\n",
        "    total_years = years + (months / 12)\n",
        "    level = 'Junior' if total_years <= 2 else ('Mid-level' if total_years <= 5 else 'Senior')\n",
        "    categorization_text = f\"**Level:** {level}\\n**Qualification:** {new_profile_dict.get('Highest Qualification', 'N/A')}\\n**Top Job Goal:** {new_profile_dict.get('Job goal 1', 'N/A')}\"\n",
        "\n",
        "    # RAG Action Plan\n",
        "    job_goal = new_profile_dict.get('Job goal 1', 'N/A'); major = new_profile_dict.get('Highest major', 'N/A')\n",
        "    system_prompt = f\"\"\"Act as an AI Career Counselor.\n",
        "    Profile: {json.dumps(new_profile_dict, indent=2)}\n",
        "    Stats: {statistical_report}\n",
        "    Weakness: {top_weakness}\n",
        "    Goal: {job_goal}\n",
        "\n",
        "    Task: Use Google Search to find courses/meetups for {job_goal} and {top_weakness}.\n",
        "    Write a comprehensive career plan with:\n",
        "    1. Analysis of hire probability.\n",
        "    2. Recommended internal workshops (from list: CV Writing, Interview Skills).\n",
        "    3. 2-3 External Courses (MUST use Google Search).\n",
        "    4. Meetups/Networking events.\n",
        "    \"\"\"\n",
        "\n",
        "    final_action_plan = await call_gemini_api(system_prompt, tools=[{\"google_search\": {}}], json_mode=False)\n",
        "\n",
        "    # Success Stories\n",
        "    hired_df = global_user_df[global_user_df['TARGET_HIRED'] == 1].copy() if not global_user_df.empty and 'TARGET_HIRED' in global_user_df.columns else pd.DataFrame()\n",
        "    stories = \"### ðŸ† Success Stories\\nNo exact matches found.\"\n",
        "    if not hired_df.empty:\n",
        "        job_goal_match = hired_df[hired_df['Job goal 1'].str.contains(job_goal, case=False, na=False)].head(3)\n",
        "        if not job_goal_match.empty:\n",
        "            stories = \"### ðŸ† Success Stories\\n\"\n",
        "            for i, row in job_goal_match.iterrows():\n",
        "                stories += f\"**ðŸ‘¤ Candidate #{row['Journey id']}** (Hired in {row['JS Town']})\\n- Role: {row['Job title 1']} - Exp: {row['Years workexperience']} years\\n\\n\"\n",
        "\n",
        "    updated_dashboard = generate_global_dashboard()\n",
        "\n",
        "    return (\n",
        "        f\"{hire_probability:.1%}\", categorization_text, individual_skill_chart, final_action_plan, final_action_plan, updated_dashboard, stories\n",
        "    )\n",
        "\n",
        "# --- 9. Follow-up Chat Function ---\n",
        "async def call_gemini_follow_up_chat(user_message: str, chat_history: list, report_context: str):\n",
        "    chat_history.append([user_message, None])\n",
        "    if not report_context:\n",
        "        chat_history[-1][1] = \"Please generate a report first.\"\n",
        "        return \"\", chat_history\n",
        "    system_prompt = f\"Context: {report_context}. User: '{user_message}'. Answer using context and Google Search.\"\n",
        "    try:\n",
        "        bot_message = await call_gemini_api(system_prompt, tools=[{\"google_search\": {}}])\n",
        "    except Exception as e:\n",
        "        bot_message = f\"Error: {e}\"\n",
        "    chat_history[-1][1] = bot_message\n",
        "    return \"\", chat_history\n",
        "\n",
        "# --- 10. Generic Gemini API Caller ---\n",
        "async def call_gemini_api(prompt, tools=None, json_mode=False):\n",
        "    apiKey = MY_GOOGLE_API_KEY\n",
        "    apiUrl = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}\"\n",
        "    generation_config = {\"responseMimeType\": \"application/json\"} if json_mode else {\"responseMimeType\": \"text/plain\"}\n",
        "    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}], \"generationConfig\": generation_config}\n",
        "    if tools: payload[\"tools\"] = tools\n",
        "\n",
        "    transport = httpx.AsyncHTTPTransport(retries=3)\n",
        "    async with httpx.AsyncClient(transport=transport) as client:\n",
        "        response = await client.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'}, timeout=90.0)\n",
        "        if response.status_code != 200: raise Exception(f\"API Error: {response.text}\")\n",
        "        result = response.json()\n",
        "        return result['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- UI DEFINITION (MULTI-PAGE LAYOUT) ---\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# AI-Powered Predictive Hiring & Action Intelligence System\")\n",
        "\n",
        "    # State variables\n",
        "    report_context_state = gr.State(value=\"\")\n",
        "    chat_history_state = gr.State(value=[])\n",
        "\n",
        "    # --- TOP LEVEL TABS ---\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # --- PAGE 1: PROFILE & ACTION PLANNING ---\n",
        "        with gr.TabItem(\"ðŸ“ Profile & Action Planning\"):\n",
        "            with gr.Row():\n",
        "                # --- LEFT COLUMN: INPUT FORM ---\n",
        "                with gr.Column(scale=3):\n",
        "                    gr.Markdown(\"### 1. Profile Input\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        resume_file = gr.File(label=\"ðŸ“„ Auto-Fill Profile from Resume (PDF)\", file_types=[\".pdf\"], scale=3)\n",
        "                        parse_btn = gr.Button(\"Parse\", scale=1)\n",
        "\n",
        "                    # FORM INPUTS MAPPING\n",
        "                    inputs_map = {}\n",
        "\n",
        "                    with gr.Accordion(\"Personal Info\", open=True):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['gender'] = gr.Dropdown(label=\"Gender\", choices=GENDER_CHOICES, value='Female')\n",
        "                            inputs_map['Age'] = gr.Number(label=\"Age\", value=25)\n",
        "                        inputs_map['Marital status'] = gr.Dropdown(label=\"Marital Status\", choices=MARITAL_CHOICES, value='Single')\n",
        "\n",
        "                    with gr.Accordion(\"Location\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['JS Town'] = gr.Textbox(label=\"Town\", value='Riyadh')\n",
        "                            inputs_map['JS Town distrinct'] = gr.Textbox(label=\"District\", value='Central')\n",
        "\n",
        "                    with gr.Accordion(\"Education & Salary\", open=False):\n",
        "                        inputs_map['Salary expectations'] = gr.Dropdown(label=\"Salary Expectations\", choices=SALARY_CHOICES, value='3,000 - 5,000SAR')\n",
        "                        inputs_map['Highest Qualification'] = gr.Dropdown(label=\"Highest Qualification\", choices=QUALIFICATION_CHOICES, value='Bachelor')\n",
        "                        inputs_map['Highest major'] = gr.Textbox(label=\"Highest Major\", value='Computer Science')\n",
        "\n",
        "                    with gr.Accordion(\"Work Preferences\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Shifts'] = gr.Dropdown(label=\"Shifts\", choices=SHIFTS_CHOICES, value='No preference')\n",
        "                            inputs_map['Working environment'] = gr.Dropdown(label=\"Environment\", choices=ENVIRONMENT_CHOICES, value='Mixed')\n",
        "\n",
        "                    with gr.Accordion(\"Skills\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Skills English'] = gr.Dropdown(label=\"English\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                            inputs_map['Skills MS Word'] = gr.Dropdown(label=\"Word\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                            inputs_map['Skills MS Excel'] = gr.Dropdown(label=\"Excel\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Skills MS Powerpoint'] = gr.Dropdown(label=\"PPT\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                            inputs_map['Communication skills'] = gr.Dropdown(label=\"Communication\", choices=COMMUNICATION_CHOICES, value='Good')\n",
        "\n",
        "                    with gr.Accordion(\"Logistics\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Transport available'] = gr.Radio(label=\"Transport?\", choices=YES_NO_CHOICES, value='No')\n",
        "                            inputs_map['Driving license'] = gr.Radio(label=\"License?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        inputs_map['Has CV (SWS)'] = gr.Radio(label=\"CV?\", choices=YES_NO_CHOICES, value='No')\n",
        "\n",
        "                    with gr.Accordion(\"Career Goals\", open=False):\n",
        "                        inputs_map['Job goal 1'] = gr.Textbox(label=\"Job Goal 1\", value='Data Analyst')\n",
        "                        inputs_map['Job goal 2'] = gr.Textbox(label=\"Job Goal 2\", value='Developer')\n",
        "                        inputs_map['Job goal 3'] = gr.Textbox(label=\"Job Goal 3\", value='IT Support')\n",
        "\n",
        "                    with gr.Accordion(\"Experience\", open=False):\n",
        "                        with gr.Row():\n",
        "                            inputs_map['Years workexperience'] = gr.Number(label=\"Years\", value=1)\n",
        "                            inputs_map['Months workexperience'] = gr.Number(label=\"Months\", value=0)\n",
        "\n",
        "                    all_inputs = [inputs_map[col] for col in ALL_FEATURE_COLUMNS]\n",
        "                    submit_btn = gr.Button(\"ðŸš€ Generate Full AI Report\", variant=\"primary\")\n",
        "                    status_output = gr.Textbox(label=\"System Status\", interactive=False, value=\"App starting...\")\n",
        "\n",
        "                # --- RIGHT COLUMN: COMMAND CENTER ---\n",
        "                with gr.Column(scale=7):\n",
        "                    gr.Markdown(\"### 2. Analysis & Recommendations Command Center\")\n",
        "                    with gr.Row():\n",
        "                        prediction_output = gr.Textbox(label=\"Hire Probability\", value=\"N/A\", scale=1)\n",
        "                        category_output = gr.Markdown(value=\"*Categorization*\")\n",
        "                        success_box = gr.Markdown(label=\"Success Stories\", value=\"*Run a report to find matches*\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        skill_chart_output = gr.Plot(label=\"Skills Radar\")\n",
        "                        action_plan_output = gr.Markdown(value=\"*Action Plan will appear here*\", label=\"AI Action Plan\")\n",
        "\n",
        "        # --- PAGE 2: MOCK INTERVIEW ---\n",
        "        with gr.TabItem(\"ðŸŽ¤ AI Mock Interview\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    role_input = gr.Textbox(label=\"Target Role\", value=\"Data Analyst\")\n",
        "                    start_interview_btn = gr.Button(\"Start New Interview\", variant=\"primary\")\n",
        "                    interview_clear_btn = gr.Button(\"Clear Chat\")\n",
        "                with gr.Column(scale=3):\n",
        "                    interview_chatbot = gr.Chatbot(height=500)\n",
        "                    interview_msg = gr.Textbox(label=\"Your Answer\")\n",
        "                    interview_send = gr.Button(\"Send Answer\")\n",
        "\n",
        "            start_interview_btn.click(start_mock_interview, inputs=[role_input, interview_chatbot], outputs=[interview_chatbot])\n",
        "            interview_msg.submit(continue_mock_interview, inputs=[interview_msg, interview_chatbot, role_input], outputs=[interview_msg, interview_chatbot])\n",
        "            interview_send.click(continue_mock_interview, inputs=[interview_msg, interview_chatbot, role_input], outputs=[interview_msg, interview_chatbot])\n",
        "            interview_clear_btn.click(lambda: [], outputs=[interview_chatbot])\n",
        "\n",
        "        # --- PAGE 3: DASHBOARD ---\n",
        "        with gr.TabItem(\"ðŸŒ Executive Analytics Dashboard\"):\n",
        "            gr.Markdown(\"### Global User Population Analysis\")\n",
        "            global_dashboard_plot = gr.Plot(label=\"Global Dashboard\")\n",
        "            refresh_button = gr.Button(\"Refresh Global Dashboard\", variant=\"secondary\")\n",
        "            refresh_button.click(generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "\n",
        "        # --- PAGE 4: CHAT ---\n",
        "        with gr.TabItem(\"ðŸ’¬ Follow-Up Counselor Chat\"):\n",
        "            gr.Markdown(\"### Ask follow-up questions about the report you just generated in Tab 1.\")\n",
        "            chat_window = gr.Chatbot(label=\"Follow-up Chat\", height=500)\n",
        "            with gr.Row():\n",
        "                chat_textbox = gr.Textbox(show_label=False, placeholder=\"Ask me anything about the report above...\", scale=8)\n",
        "                chat_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "            chat_btn.click(call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "            chat_textbox.submit(call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "\n",
        "\n",
        "    # --- FINAL GLOBAL CONNECTIONS ---\n",
        "    demo.load(fn=on_app_load, outputs=[status_output]).then(fn=generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "    parse_btn.click(parse_resume, inputs=[resume_file] + all_inputs, outputs=all_inputs)\n",
        "    submit_btn.click(\n",
        "        fn=process_new_user_submission,\n",
        "        inputs=all_inputs,\n",
        "        outputs=[prediction_output, category_output, skill_chart_output, action_plan_output, report_context_state, global_dashboard_plot, success_box]\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Launching Ultimate App...\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "2Iy_4NJ-ou_Y",
        "outputId": "2ea2520f-a494-4330-a00e-2ce0d093039b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\n",
            "Launching Ultimate App...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ab67d4af6d65258294.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ab67d4af6d65258294.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\n",
            "...XGBoost Models loaded successfully.\n",
            "...Loaded 2000 users for Global Dashboard.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ab67d4af6d65258294.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the application\n",
        "import datetime\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import json\n",
        "import httpx\n",
        "import asyncio\n",
        "from google.colab import userdata\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "print(\"--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\")\n",
        "\n",
        "# --- 1. Load API Key from Colab Secrets ---\n",
        "try:\n",
        "    MY_GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    if not MY_GOOGLE_API_KEY:\n",
        "        raise ValueError(\"API Key is empty or not found.\")\n",
        "    print(\"Successfully loaded Google API Key from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"---!!! CRITICAL ERROR: API KEY NOT FOUND: {e} !!!---\")\n",
        "    print(\"Please go to the 'Key' (ðŸ”‘) icon on the left and add your API key as a secret named 'GOOGLE_API_KEY'\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Define Global Variables & AI Models ---\n",
        "ai_models = {\"pipeline\": None, \"explainer\": None, \"feature_names\": None}\n",
        "global_user_df = pd.DataFrame()\n",
        "\n",
        "ALL_FEATURE_COLUMNS = [\n",
        "    'gender', 'Age', 'Marital status', 'JS Town', 'JS Town distrinct',\n",
        "    'Salary expectations', 'Highest Qualification', 'Highest major',\n",
        "    'Shifts', 'Working environment', 'Skills English', 'Skills MS Word',\n",
        "    'Skills MS Excel', 'Skills MS Powerpoint', 'Transport available', 'Driving license',\n",
        "    'Job goal 1', 'Job goal 2', 'Job goal 3', 'Years workexperience',\n",
        "    'Months workexperience', 'Communication skills', 'Has CV (SWS)'\n",
        "]\n",
        "\n",
        "try:\n",
        "    if 'master_df' in locals() and not master_df.empty:\n",
        "        initial_profile_dict = {col: \"Unknown\" if not np.issubdtype(master_df[col].dtype, np.number) else 0 for col in ALL_FEATURE_COLUMNS}\n",
        "    else:\n",
        "        initial_profile_dict = {col: 0 if col in ['Age', 'Years workexperience', 'Months workexperience'] else 'Unknown' for col in ALL_FEATURE_COLUMNS}\n",
        "except NameError:\n",
        "    initial_profile_dict = {col: 0 if col in ['Age', 'Years workexperience', 'Months workexperience'] else 'Unknown' for col in ALL_FEATURE_COLUMNS}\n",
        "\n",
        "\n",
        "# --- 3. Define Dropdown Choices ---\n",
        "GENDER_CHOICES = ['Male', 'Female', 'Other', 'Prefer not to say']\n",
        "MARITAL_CHOICES = ['Single', 'Married', 'Divorced', 'Widowed', 'Other']\n",
        "SALARY_CHOICES = ['< 3,000SAR', '3,000 - 5,000SAR', '5,000 - 7,000SAR', '> 7,000SAR']\n",
        "QUALIFICATION_CHOICES = ['High School', 'Diploma', 'Bachelor', 'Masters', 'Doctorate']\n",
        "SHIFTS_CHOICES = ['No preference', 'Straight shifts', 'Rotating shifts']\n",
        "ENVIRONMENT_CHOICES = ['Mixed', 'Flexible', 'On-site', 'Remote']\n",
        "SKILL_LEVEL_CHOICES = ['N_A', 'Beginner', 'Good', 'Excellent']\n",
        "YES_NO_CHOICES = ['Yes', 'No']\n",
        "COMMUNICATION_CHOICES = ['Beginner', 'Good', 'Excellent']\n",
        "\n",
        "\n",
        "# --- 4. Load XGBoost AI Brain ---\n",
        "def on_app_load():\n",
        "    global ai_models, global_user_df\n",
        "    print(\"Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\")\n",
        "    try:\n",
        "        ai_models[\"pipeline\"] = joblib.load(BRAIN_FILE_PATH)\n",
        "        ai_models[\"explainer\"] = joblib.load(EXPLAINER_FILE_PATH)\n",
        "        preprocessor = ai_models[\"pipeline\"].named_steps['preprocessor']\n",
        "        ai_models[\"feature_names\"] = preprocessor.get_feature_names_out().tolist()\n",
        "        print(\"...XGBoost Models loaded successfully.\")\n",
        "        global_user_df = pd.read_csv(MASTER_DATASET_PATH)\n",
        "        print(f\"...Loaded {len(global_user_df)} users for Global Dashboard.\")\n",
        "        return \"System Ready. AI Models and Global Data loaded.\"\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! CRITICAL ERROR LOADING MODELS: {e} !!!---\")\n",
        "        return f\"CRITICAL ERROR: Could not load AI models or data. {e}\"\n",
        "\n",
        "# --- 5. Helper Functions ---\n",
        "def format_feature_name_for_llm(feature_name):\n",
        "    name = feature_name.replace(\"cat__\", \"\").replace(\"ord__\", \"\").replace(\"num__\", \"\")\n",
        "    parts = name.split('_', 1)\n",
        "    if len(parts) == 2:\n",
        "        if \"Skills\" in parts[0]: return parts[0].replace(\"Skills \", \"\")\n",
        "        if \"Job goal\" in parts[0]: return \"Job Goal Setting\"\n",
        "        return parts[0]\n",
        "    return name\n",
        "\n",
        "def run_xgb_prediction_and_get_report(profile_dict):\n",
        "    print(\"XGBoost Brain: Running prediction...\")\n",
        "    try:\n",
        "        if ai_models[\"pipeline\"] is None: return \"Error: XGBoost model not loaded.\", \"Unknown\", 0.0\n",
        "        new_user_df = pd.DataFrame([profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "        pipeline = ai_models[\"pipeline\"]\n",
        "        explainer = ai_models[\"explainer\"]\n",
        "        feature_names = ai_models[\"feature_names\"]\n",
        "        prediction_proba = pipeline.predict_proba(new_user_df)[0]\n",
        "        hire_probability = prediction_proba[1]\n",
        "        user_transformed = pipeline.named_steps['preprocessor'].transform(new_user_df)\n",
        "        shap_values = explainer.shap_values(user_transformed)\n",
        "        df_shap = pd.DataFrame(shap_values, columns=feature_names).iloc[0].T\n",
        "        df_shap = df_shap.reset_index(); df_shap.columns = ['Feature', 'SHAP_Value']\n",
        "        df_shap['abs_impact'] = df_shap['SHAP_Value'].abs()\n",
        "        df_negatives = df_shap[df_shap['SHAP_Value'] < 0].sort_values(by='SHAP_Value', ascending=True)\n",
        "        top_weakness_topic = \"General Profile Improvement\"\n",
        "        if not df_negatives.empty:\n",
        "            top_weakness_internal_name = df_negatives.iloc[0]['Feature']\n",
        "            top_weakness_topic = format_feature_name_for_llm(top_weakness_internal_name)\n",
        "        report = f\"STATISTICAL ANALYSIS REPORT:\\n\"\n",
        "        report += f\"Predicted Hire Probability: {hire_probability:.1%}\\n\"\n",
        "        report += \"Top 5 Most Important Factors:\\n\"\n",
        "        for _, row in df_shap.sort_values(by='abs_impact', ascending=False).head(5).iterrows():\n",
        "            sign = \"POSITIVE\" if row['SHAP_Value'] > 0 else \"NEGATIVE\"\n",
        "            report += f\"  - Factor: {row['Feature']}, Impact: {sign}\\n\"\n",
        "        print(f\"...XGBoost Brain: Report generated. Top weakness: {top_weakness_topic}\")\n",
        "        return report, top_weakness_topic, hire_probability\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR during XGBoost prediction: {e} !!!---\")\n",
        "        return f\"Error during XGBoost prediction: {e}\", \"Unknown\", 0.0\n",
        "\n",
        "# --- 6. Generate Individual Charts ---\n",
        "def generate_individual_charts(profile_dict):\n",
        "    print(\"Generating individual skills radar chart...\")\n",
        "    skills = {\n",
        "        'English': profile_dict.get('Skills English', 'N_A'),\n",
        "        'MS Word': profile_dict.get('Skills MS Word', 'N_A'),\n",
        "        'MS Excel': profile_dict.get('Skills MS Excel', 'N_A'),\n",
        "        'MS Powerpoint': profile_dict.get('Skills MS Powerpoint', 'N_A'),\n",
        "        'Communication': profile_dict.get('Communication skills', 'N_A')\n",
        "    }\n",
        "    level_map = {'N_A': 0, 'Beginner': 1, 'Good': 2, 'Excellent': 3}\n",
        "    values = [level_map.get(v, 0) for v in skills.values()]\n",
        "    labels = list(skills.keys())\n",
        "    N = len(labels)\n",
        "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
        "    values += values[:1]\n",
        "    angles += angles[:1]\n",
        "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
        "    ax.fill(angles, values, color='blue', alpha=0.25)\n",
        "    ax.plot(angles, values, color='blue', linewidth=2)\n",
        "    ax.set_yticklabels(['', 'Beginner', 'Good', 'Excellent'])\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_title(\"Candidate Skills Profile\", size=15)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# --- 7. Generate Global Dashboard ---\n",
        "def generate_global_dashboard():\n",
        "    global global_user_df\n",
        "    print(\"Generating global dashboard...\")\n",
        "    if global_user_df.empty:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.text(0.5, 0.5, \"Global user data not loaded.\", ha='center')\n",
        "        return fig\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    gs = fig.add_gridspec(2, 3) # 2 rows, 3 columns\n",
        "\n",
        "    try:\n",
        "        # Chart 1: Domain\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        domain_counts = global_user_df['Stream'].value_counts()\n",
        "        ax1.bar(domain_counts.index, domain_counts.values, color='teal')\n",
        "        ax1.set_title('Candidates by Domain (Stream)')\n",
        "        ax1.tick_params(axis='x', rotation=25)\n",
        "\n",
        "        # Chart 2: Scatter Plot\n",
        "        ax2 = fig.add_subplot(gs[0, 1:])\n",
        "        years = pd.to_numeric(global_user_df['Years workexperience'].fillna(0), errors='coerce').fillna(0)\n",
        "        months = pd.to_numeric(global_user_df['Months workexperience'].fillna(0), errors='coerce').fillna(0)\n",
        "        global_user_df['TotalYears'] = years + (months / 12)\n",
        "\n",
        "        streams = global_user_df['Stream'].dropna().unique()\n",
        "        stream_map = {stream: i for i, stream in enumerate(streams)}\n",
        "        global_user_df['Stream_Num'] = global_user_df['Stream'].map(stream_map)\n",
        "\n",
        "        statuses = global_user_df['Work status'].unique()\n",
        "        for i, status in enumerate(statuses):\n",
        "            subset = global_user_df[global_user_df['Work status'] == status]\n",
        "            ax2.scatter(\n",
        "                subset['Stream'].map(stream_map) + np.random.normal(0, 0.1, len(subset)),\n",
        "                subset['TotalYears'],\n",
        "                label=status, alpha=0.6, edgecolors='w'\n",
        "            )\n",
        "        ax2.set_xticks(range(len(streams)))\n",
        "        ax2.set_xticklabels(streams)\n",
        "        ax2.set_title('Distribution of Experience by Domain (Colored by Work Status)')\n",
        "        ax2.legend(title=\"Status\")\n",
        "        ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "        # Chart 3: Qualification\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        qual_counts = global_user_df['Highest Qualification'].value_counts().head(7).sort_values()\n",
        "        ax3.barh(qual_counts.index, qual_counts.values, color='mediumpurple')\n",
        "        ax3.set_title('Highest Qualification')\n",
        "\n",
        "        # Chart 4: Gender\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        gender_counts = global_user_df['gender'].value_counts()\n",
        "        ax4.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%',\n",
        "               colors=['lightblue', 'lightpink', 'lightgrey'], startangle=90)\n",
        "        ax4.set_title('Gender')\n",
        "\n",
        "        # Chart 5: Goals\n",
        "        ax5 = fig.add_subplot(gs[1, 2])\n",
        "        job_goal_counts = global_user_df['Job goal 1'].value_counts().head(5)\n",
        "        ax5.barh(job_goal_counts.index, job_goal_counts.values, color='gold')\n",
        "        ax5.set_title('Top 5 Job Goals')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating global dashboard: {e}\")\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.text(0.5, 0.5, f\"Error generating charts: {e}\", ha='center')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# --- 8. Submit Function ---\n",
        "async def process_new_user_submission(*args):\n",
        "    print(\"New user submitted. Processing...\")\n",
        "    if ai_models[\"pipeline\"] is None:\n",
        "        return \"ERROR: AI Models not loaded.\", \"Error\", None, \"Error\", None, None\n",
        "\n",
        "    new_profile_dict = dict(zip(ALL_FEATURE_COLUMNS, args))\n",
        "\n",
        "    try:\n",
        "        save_data = pd.DataFrame([new_profile_dict], columns=ALL_FEATURE_COLUMNS)\n",
        "        save_data['Journey id'] = f\"WEB-{int(datetime.datetime.now().timestamp())}\"\n",
        "        save_data['Attachment date'] = datetime.date.today().isoformat()\n",
        "        save_data['Stream'] = 'Online Submission'\n",
        "        save_data['Work status'] = 'On programme'\n",
        "        file_exists = os.path.isfile(NEW_SUBMISSIONS_FILE_PATH)\n",
        "        save_data.to_csv(NEW_SUBMISSIONS_FILE_PATH, mode='a', header=not file_exists, index=False)\n",
        "        print(\"Profile saved to new_submissions.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- ERROR Saving to Drive: {e} ---\")\n",
        "\n",
        "    statistical_report, top_weakness, hire_probability = run_xgb_prediction_and_get_report(new_profile_dict)\n",
        "    individual_skill_chart = generate_individual_charts(new_profile_dict)\n",
        "\n",
        "    years = new_profile_dict.get('Years workexperience', 0)\n",
        "    total_years = float(years) + (float(new_profile_dict.get('Months workexperience', 0)) / 12)\n",
        "    if total_years <= 2: level = 'Junior'\n",
        "    elif total_years <= 5: level = 'Mid-level'\n",
        "    else: level = 'Senior'\n",
        "\n",
        "    categorization_text = f\"**Profile Category:**\\n\"\n",
        "    categorization_text += f\"  - **Level:** {level}\\n\"\n",
        "    categorization_text += f\"  - **Qualification:** {new_profile_dict.get('Highest Qualification', 'N/A')}\\n\"\n",
        "    categorization_text += f\"  - **Top Job Goal:** {new_profile_dict.get('Job goal 1', 'N/A')}\"\n",
        "\n",
        "    print(\"AI Counselor: Calling Gemini for RAG Action Plan...\")\n",
        "    job_goal = new_profile_dict.get('Job goal 1', 'N/A')\n",
        "    major = new_profile_dict.get('Highest major', 'N/A')\n",
        "\n",
        "    search_query_1 = f\"free online courses for {job_goal} with {major} background\"\n",
        "    search_query_2 = f\"best paid certificate programs on Coursera or Udemy for {job_goal}\"\n",
        "    search_query_3 = f\"professional networking groups or meetups for {job_goal} in Saudi Arabia\"\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an expert AI Career Counselor. A candidate has just completed their profile.\n",
        "    Your job is to write a comprehensive, encouraging, and actionable career plan.\n",
        "\n",
        "    HERE IS THE DATA YOU MUST USE:\n",
        "    1. THE CANDIDATE'S PROFILE: {json.dumps(new_profile_dict, indent=2)}\n",
        "    2. THE STATISTICAL ANALYSIS REPORT (from our predictive model): {statistical_report}\n",
        "    3. A LIST OF *INTERNAL* WORKSHOPS:\n",
        "       - \"Work ethics and an introduction to labor culture\"\n",
        "       - \"Initial appointment / Induction\", \"CV Writing\", \"Interview Skills\"\n",
        "\n",
        "    YOUR TASK:\n",
        "    1.  **Analyze:** Read the STATISTICAL REPORT. Identify the candidate's hire probability and their biggest weakness.\n",
        "    2.  **Search:** **USE THE GOOGLE SEARCH TOOL** to find real-world resources. You will perform 3 searches:\n",
        "        * Search 1: `{search_query_1}` (for free courses)\n",
        "        * Search 2: `{search_query_2}` (for paid certificates)\n",
        "        * Search 3: `{search_query_3}` (for meetups/networking)\n",
        "    4.  **Synthesize Plan:** Write a comprehensive, multi-part career plan.\n",
        "        * **Introduction:** Start by being encouraging.\n",
        "        * **Part 1: Immediate Profile Improvement (The Weakness):** Use the STATISTICAL REPORT to explain their *biggest statistical weakness*.\n",
        "        * **Part 2: Internal Workshops:** Recommend one *internal workshop* from the list.\n",
        "        * **Part 3: External Training Plan (Free):** Recommend 1-2 **free courses** you found from Search 1. **You MUST include the source title and the URL.**\n",
        "        * **Part 4: External Training Plan (Paid/Certificate):** Recommend 1-2 **paid certificate programs** from Search 2. **You MUST include the source title and the URL.**\n",
        "        * **Part 5: Community & Networking:** Recommend 1-2 **networking groups or meetups** from Search 3. **You MUST include the source title and the URL.**\n",
        "        * **Conclusion:** End with an encouraging closing statement.\n",
        "    \"\"\"\n",
        "\n",
        "    apiKey = MY_GOOGLE_API_KEY\n",
        "    apiUrl = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": \"Generate the career plan.\"}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\"responseMimeType\": \"text/plain\"},\n",
        "        \"tools\": [{\"google_search\": {}}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        transport = httpx.AsyncHTTPTransport(retries=3)\n",
        "        async with httpx.AsyncClient(transport=transport) as client:\n",
        "            response = await client.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'}, timeout=90.0)\n",
        "\n",
        "        if not response.status_code == 200:\n",
        "            raise Exception(f\"RAG API call failed: {response.text}\")\n",
        "\n",
        "        result = response.json()\n",
        "        if not result.get('candidates'): raise Exception(\"Invalid RAG response\")\n",
        "\n",
        "        final_action_plan = result['candidates'][0]['content']['parts'][0]['text']\n",
        "        print(\"...RAG Action Plan generated.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"---!!! ERROR Calling RAG API: {e} !!!---\")\n",
        "        final_action_plan = f\"An error occurred while generating the RAG action plan: {e}\"\n",
        "\n",
        "    updated_dashboard = generate_global_dashboard()\n",
        "\n",
        "    return (\n",
        "        f\"{hire_probability:.1%}\",\n",
        "        categorization_text,\n",
        "        individual_skill_chart,\n",
        "        final_action_plan,\n",
        "        final_action_plan,\n",
        "        updated_dashboard\n",
        "    )\n",
        "\n",
        "# --- 9. Follow-up Chat Function ---\n",
        "async def call_gemini_follow_up_chat(user_message: str, chat_history: list, report_context: str):\n",
        "    chat_history.append([user_message, None])\n",
        "    if not report_context:\n",
        "        chat_history[-1][1] = \"I'm sorry, you must generate a report in the 'New Candidate Report' tab first.\"\n",
        "        return \"\", chat_history\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an AI Career Counselor. Context:\n",
        "    {report_context}\n",
        "    User question: \"{user_message}\"\n",
        "    Answer the question using the report context and Google Search if needed.\n",
        "    \"\"\"\n",
        "\n",
        "    apiKey = MY_GOOGLE_API_KEY\n",
        "    apiUrl = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_message}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\"responseMimeType\": \"text/plain\"},\n",
        "        \"tools\": [{\"google_search\": {}}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        transport = httpx.AsyncHTTPTransport(retries=3)\n",
        "        async with httpx.AsyncClient(transport=transport) as client:\n",
        "            response = await client.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'}, timeout=90.0)\n",
        "        if not response.status_code == 200: raise Exception(f\"API failed: {response.text}\")\n",
        "        result = response.json()\n",
        "        bot_message = result['candidates'][0]['content']['parts'][0]['text']\n",
        "    except Exception as e:\n",
        "        bot_message = f\"Error: {e}\"\n",
        "\n",
        "    chat_history[-1][1] = bot_message\n",
        "    return \"\", chat_history\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- END OF LOGIC / START OF FULL PAGE UI (MULTI-PAGE TABS) ---\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "print(\"--- Phase 5: Launching the Gradio MULTI-PAGE App ---\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# AI-Powered Predictive Hiring & Action Intelligence System\")\n",
        "\n",
        "    report_context_state = gr.State(value=\"\")\n",
        "    chat_history_state = gr.State(value=[])\n",
        "\n",
        "    # --- TOP LEVEL TABS ---\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # --- PAGE 1: AI COUNSELOR ---\n",
        "        with gr.TabItem(\"ðŸ¤– AI Counselor & Prediction\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=3):\n",
        "                    gr.Markdown(\"### 1. Candidate Profile Input\")\n",
        "                    all_inputs = []\n",
        "                    with gr.Accordion(\"Personal Info\", open=True):\n",
        "                        with gr.Row():\n",
        "                            gender_input = gr.Dropdown(label=\"Gender\", choices=GENDER_CHOICES, value='Female')\n",
        "                            age_input = gr.Number(label=\"Age\", value=25)\n",
        "                        marital_input = gr.Dropdown(label=\"Marital Status\", choices=MARITAL_CHOICES, value='Single')\n",
        "                        all_inputs.extend([gender_input, age_input, marital_input])\n",
        "\n",
        "                    with gr.Accordion(\"Location\", open=False):\n",
        "                        town_input = gr.Textbox(label=\"Town\", value='Riyadh')\n",
        "                        district_input = gr.Textbox(label=\"District\", value='Central')\n",
        "                        all_inputs.extend([town_input, district_input])\n",
        "\n",
        "                    with gr.Accordion(\"Education & Salary\", open=False):\n",
        "                        salary_input = gr.Dropdown(label=\"Salary Expectations\", choices=SALARY_CHOICES, value='3,000 - 5,000SAR')\n",
        "                        qual_input = gr.Dropdown(label=\"Highest Qualification\", choices=QUALIFICATION_CHOICES, value='Bachelor')\n",
        "                        major_input = gr.Textbox(label=\"Highest Major\", value='Computer Science')\n",
        "                        all_inputs.extend([salary_input, qual_input, major_input])\n",
        "\n",
        "                    with gr.Accordion(\"Work Preferences\", open=False):\n",
        "                        shifts_input = gr.Dropdown(label=\"Shifts\", choices=SHIFTS_CHOICES, value='No preference')\n",
        "                        env_input = gr.Dropdown(label=\"Environment\", choices=ENVIRONMENT_CHOICES, value='Mixed')\n",
        "                        all_inputs.extend([shifts_input, env_input])\n",
        "\n",
        "                    with gr.Accordion(\"Skills\", open=False):\n",
        "                        eng_input = gr.Dropdown(label=\"English\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        word_input = gr.Dropdown(label=\"Word\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        excel_input = gr.Dropdown(label=\"Excel\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        ppt_input = gr.Dropdown(label=\"PPT\", choices=SKILL_LEVEL_CHOICES, value='Good')\n",
        "                        comm_input = gr.Dropdown(label=\"Communication\", choices=COMMUNICATION_CHOICES, value='Good')\n",
        "                        all_inputs.extend([eng_input, word_input, excel_input, ppt_input])\n",
        "\n",
        "                    with gr.Accordion(\"Logistics\", open=False):\n",
        "                        transport_input = gr.Radio(label=\"Transport?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        license_input = gr.Radio(label=\"License?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        cv_input = gr.Radio(label=\"CV?\", choices=YES_NO_CHOICES, value='No')\n",
        "                        all_inputs.extend([transport_input, license_input])\n",
        "\n",
        "                    with gr.Accordion(\"Career Goals\", open=False):\n",
        "                        goal1_input = gr.Textbox(label=\"Job Goal 1\", value='Data Analyst')\n",
        "                        goal2_input = gr.Textbox(label=\"Job Goal 2\", value='Developer')\n",
        "                        goal3_input = gr.Textbox(label=\"Job Goal 3\", value='IT Support')\n",
        "                        all_inputs.extend([goal1_input, goal2_input, goal3_input])\n",
        "\n",
        "                    with gr.Accordion(\"Experience\", open=False):\n",
        "                        with gr.Row():\n",
        "                            years_exp_input = gr.Number(label=\"Years\", value=1)\n",
        "                            months_exp_input = gr.Number(label=\"Months\", value=0)\n",
        "                        all_inputs.extend([years_exp_input, months_exp_input])\n",
        "\n",
        "                    all_inputs.append(comm_input)\n",
        "                    all_inputs.append(cv_input)\n",
        "\n",
        "                    submit_btn = gr.Button(\"Generate AI Report\", variant=\"primary\")\n",
        "                    status_output = gr.Textbox(label=\"Status\", interactive=False, value=\"App starting...\")\n",
        "\n",
        "                with gr.Column(scale=7):\n",
        "                    gr.Markdown(\"### 2. AI Command Center\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        prediction_output = gr.Textbox(label=\"Hire Probability\", value=\"N/A\", scale=1)\n",
        "                        category_output = gr.Markdown(value=\"*Categorization*\") # Removed 'scale' arg\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=1):\n",
        "                            skill_chart_output = gr.Plot(label=\"Skills Radar\")\n",
        "                        with gr.Column(scale=2):\n",
        "                            action_plan_output = gr.Markdown(value=\"*Action Plan will appear here*\", label=\"AI Action Plan\")\n",
        "\n",
        "                    gr.Markdown(\"### 3. AI Counselor Chat\")\n",
        "                    chat_window = gr.Chatbot(label=\"Ask follow-up questions about the plan...\", height=300)\n",
        "                    with gr.Row():\n",
        "                        chat_textbox = gr.Textbox(show_label=False, placeholder=\"Ask me anything about the report above...\", scale=8)\n",
        "                        chat_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "        # --- PAGE 2: DASHBOARD ---\n",
        "        with gr.TabItem(\"ðŸ“Š Executive Analytics Dashboard\"):\n",
        "            gr.Markdown(\"### Global User Population Analysis\")\n",
        "            global_dashboard_plot = gr.Plot(label=\"Global Dashboard\")\n",
        "            refresh_button = gr.Button(\"Refresh Global Dashboard\", variant=\"secondary\")\n",
        "\n",
        "\n",
        "    # --- CONNECTING FUNCTIONS ---\n",
        "    demo.load(fn=on_app_load, outputs=[status_output]).then(fn=generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_new_user_submission,\n",
        "        inputs=all_inputs,\n",
        "        outputs=[prediction_output, category_output, skill_chart_output, action_plan_output, report_context_state, global_dashboard_plot]\n",
        "    )\n",
        "\n",
        "    refresh_button.click(fn=generate_global_dashboard, outputs=[global_dashboard_plot])\n",
        "\n",
        "    chat_btn.click(fn=call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "    chat_textbox.submit(fn=call_gemini_follow_up_chat, inputs=[chat_textbox, chat_window, report_context_state], outputs=[chat_textbox, chat_window])\n",
        "\n",
        "print(\"Launching...\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "K953fvxplKX8",
        "outputId": "eaa8d06f-7666-4c0b-d7d9-597322a09cdc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 4 & 5: Defining App Logic and Launching Multi-Page UI ---\n",
            "Successfully loaded Google API Key from Colab Secrets.\n",
            "--- Phase 5: Launching the Gradio MULTI-PAGE App ---\n",
            "Launching...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3f1fef629d72f2aebc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3f1fef629d72f2aebc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio App Loading: Loading *XGBoost* AI models from Google Drive...\n",
            "...XGBoost Models loaded successfully.\n",
            "...Loaded 2000 users for Global Dashboard.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3f1fef629d72f2aebc.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8"
      ],
      "metadata": {
        "id": "JhkC_YkFPpJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9"
      ],
      "metadata": {
        "id": "6K-luV5yPuQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10"
      ],
      "metadata": {
        "id": "ylmnRy7SPwoH"
      }
    }
  ]
}